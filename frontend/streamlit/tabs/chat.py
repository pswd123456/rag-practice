# frontend/tabs/chat.py
import streamlit as st
import api

def render_chat_tab(selected_kb, current_session):
    """
    æ¸²æŸ“å¯¹è¯ç•Œé¢
    """
    if not current_session:
        st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§æ–°å»ºæˆ–é€‰æ‹©ä¸€ä¸ªä¼šè¯ä»¥å¼€å§‹å¯¹è¯ã€‚")
        return

    # 1. é¡¶éƒ¨é…ç½®
    col_s1, col_s2, col_s3 = st.columns([1.5, 1.5, 2])
    with col_s1:
        st.caption(f"å½“å‰ä¼šè¯: **{current_session['title']}**")
    with col_s2:
        llm_model = st.selectbox(
            "æ¨¡å‹", 
            ["qwen-flash", "qwen-plus", "qwen-max", "deepseek-chat"],
            index=2,
            label_visibility="collapsed"
        )
    with col_s3:
        top_k = st.slider("Recall TopK", 1, 10, 5, label_visibility="collapsed")

    st.divider()

    # 2. åŠ è½½å¹¶æ˜¾ç¤ºå†å²æ¶ˆæ¯
    # ä¸å†ä¾èµ– st.session_state.messages æ¥æŒä¹…åŒ–ï¼Œè€Œæ˜¯æ¯æ¬¡é‡ç»˜éƒ½ä»åç«¯æ‹‰å–
    # ä½†ä¸ºäº†æµå¼ä½“éªŒï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ session_state åšä¸´æ—¶ç¼“å­˜ï¼Œæˆ–è€…ç›´æ¥ç›¸ä¿¡åç«¯é€Ÿåº¦
    
    # ç­–ç•¥ï¼šåˆå§‹åŒ–æ—¶æ‹‰å–ä¸€æ¬¡ï¼Œåç»­æµå¼è¿½åŠ åˆ°æœ¬åœ° stateï¼Œrerun åå†æ¬¡æ‹‰å–è¦†ç›–
    if "messages" not in st.session_state or st.session_state.get("current_session_id") != current_session["id"]:
        # ä¼šè¯åˆ‡æ¢äº†ï¼Œé‡æ–°æ‹‰å–
        with st.spinner("æ­£åœ¨åŠ è½½å†å²è®°å½•..."):
            msgs = api.get_session_messages(current_session["id"])
            st.session_state.messages = msgs
            st.session_state.current_session_id = current_session["id"]
    
    # æ¸²æŸ“å†å²
    for msg in st.session_state.messages:
        role = msg["role"]
        content = msg["content"]
        sources = msg.get("sources", [])
        
        with st.chat_message(role):
            st.markdown(content)
            if sources:
                with st.expander(f"ğŸ“š å‚è€ƒäº† {len(sources)} ä¸ªåˆ‡ç‰‡"):
                    for idx, src in enumerate(sources):
                        score = f"(Score: {src.get('score', 0):.2f})" if src.get('score') else ""
                        st.markdown(f"**[{idx+1}] {src['filename']}** {score}")
                        st.caption(src['content'])

    # 3. å¤„ç†è¾“å…¥
    if prompt := st.chat_input("è¾“å…¥é—®é¢˜..."):
        # 3.1 ç«‹å³æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
            
        # 3.2 è°ƒç”¨æµå¼æ¥å£
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            retrieved_sources = []
            
            payload = {
                "query": prompt,
                "top_k": top_k,
                "llm_model": llm_model,
                "stream": True
            }
            
            # ä½¿ç”¨ç”Ÿæˆå™¨
            stream_gen = api.chat_completion_stream(current_session["id"], payload)
            
            try:
                for event in stream_gen:
                    if "error" in event:
                        st.error(event["error"])
                        break
                    
                    if event["type"] == "message":
                        chunk = event["data"]
                        full_response += chunk
                        message_placeholder.markdown(full_response + "â–Œ")
                    
                    elif event["type"] == "sources":
                        retrieved_sources = event["data"]
            except Exception as e:
                st.error(f"Stream Error: {e}")

            message_placeholder.markdown(full_response)
            
            # æ˜¾ç¤ºæ¥æº
            if retrieved_sources:
                with st.expander(f"ğŸ“š å‚è€ƒäº† {len(retrieved_sources)} ä¸ªåˆ‡ç‰‡"):
                    for idx, src in enumerate(retrieved_sources):
                        st.markdown(f"**[{idx+1}] {src['filename']}**")
                        st.caption(src['content'])
            
            # 3.3 æ›´æ–°æœ¬åœ° State (é˜²æ­¢ç”¨æˆ·æ‰‹åŠ¨åˆ·æ–°å‰æ•°æ®ä¸¢å¤±)
            st.session_state.messages.append({
                "role": "assistant",
                "content": full_response,
                "sources": retrieved_sources
            })