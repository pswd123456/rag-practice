import streamlit as st
import api

def render_chat_tab(selected_kb):
    # 1. é¡¶éƒ¨é…ç½®åŒº
    # [Modify] è°ƒæ•´åˆ—å®½æ¯”ä¾‹é€‚é…æ–°æ§ä»¶
    col_s1, col_s2, col_s3 = st.columns([1.2, 1.2, 2.6])
    
    with col_s1:
        llm_model = st.selectbox(
            "å¯¹è¯æ¨¡å‹ (Generator)", 
            [
                "qwen-flash", 
                "qwen-plus", 
                "qwen-max", 
                "deepseek-chat", 
                "deepseek-reasoner",
                "google/gemini-3-pro-preview-free"
            ],
            index=0,
            help="è´Ÿè´£æ ¹æ®æ£€ç´¢ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”çš„æ¨¡å‹"
        )
        
    with col_s2:
        # [Modify] ç§»é™¤ Strategy é€‰æ‹©ï¼Œæ”¹ä¸º Final Top K æ§åˆ¶
        # è¿™æ˜¯ Rerank ä¹‹åæœ€ç»ˆä¿ç•™ç»™ LLM çš„æ–‡æ¡£æ•°é‡
        top_k = st.number_input(
            "Final Top K", 
            min_value=1, 
            max_value=10, 
            value=5, 
            help="é‡æ’åºåï¼Œæœ€ç»ˆä¿ç•™å¹¶å–‚ç»™ LLM çš„æ–‡æ¡£æ•°é‡ (Recall é»˜è®¤ä¸º 50)"
        )

    with col_s3:
        # [Modify] ä¼˜åŒ–å¸ƒå±€ï¼Œæ˜¾ç¤º Rerank çŠ¶æ€
        use_stream = st.checkbox("æµå¼è¾“å‡º", value=True)
        st.caption("ğŸš€ Rerank: `Enabled` (bge-reranker-v2-m3)")
    
    st.divider()

    if "messages" not in st.session_state:
        st.session_state.messages = []
        
    if st.button("ğŸ§¹ æ¸…ç©ºå¯¹è¯"):
        st.session_state.messages = []
        st.rerun()

    # 2. æ¸²æŸ“å†å²æ¶ˆæ¯
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            if "sources" in msg and msg["sources"]:
                with st.expander(f"ğŸ“š å‚è€ƒäº† {len(msg['sources'])} ä¸ªåˆ‡ç‰‡"):
                    for idx, src in enumerate(msg["sources"]):
                        # [Opt] æ˜¾ç¤º Rerank åˆ†æ•° (å¦‚æœæœ‰)
                        score_info = ""
                        if "score" in src:
                            score_info = f" (Score: {src['score']:.4f})"
                            
                        page_num = src.get("page_number")
                        page_info = f" (P{page_num})" if page_num else ""
                        
                        st.markdown(f"**[{idx+1}] {src['source_filename']}{page_info}**{score_info}")
                        st.caption(src['chunk_content'])

    # 3. å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("åœ¨è¿™ä¸ªçŸ¥è¯†åº“ä¸­æœç´¢..."):
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # æ˜¾ç¤ºåŠ©æ‰‹å“åº”
        with st.chat_message("assistant"):
            # [Modify] æ„å»ºæ–°çš„ Payloadï¼Œç§»é™¤ strategyï¼Œæ·»åŠ  top_k
            payload = {
                "query": prompt,
                "knowledge_id": selected_kb['id'],
                "top_k": top_k,          # Rerank åçš„æˆªæ–­æ•°é‡
                "llm_model": llm_model   # é€‰ä¸­çš„ç”Ÿæˆæ¨¡å‹
                # "rerank_model_name": ... # å¯é€‰ï¼Œä¸ä¼ åˆ™ä½¿ç”¨åç«¯é»˜è®¤é…ç½®
            }
            
            full_response = ""
            retrieved_sources = []

            # ================= A. æµå¼æ¨¡å¼ =================
            if use_stream:
                message_placeholder = st.empty()
                stream_gen = api.chat_stream(payload)
                
                for event in stream_gen:
                    if "error" in event:
                        message_placeholder.error(event["error"])
                        full_response = "Error"
                        break
                    
                    if event["type"] == "sources":
                        retrieved_sources = event["data"]
                    
                    elif event["type"] == "message":
                        full_response += event["data"]
                        message_placeholder.markdown(full_response + "â–Œ")
                
                if full_response != "Error":
                    message_placeholder.markdown(full_response)

            # ================= B. æ™®é€šæ¨¡å¼ =================
            else:
                with st.spinner(f"æ­£åœ¨æ£€ç´¢ä¸æ€è€ƒ ({llm_model})..."):
                    success, data = api.chat_query(payload)
                    if success:
                        full_response = data["answer"]
                        retrieved_sources = data["sources"]
                        st.markdown(full_response)
                    else:
                        st.error(data)
                        full_response = "Error"

            # ================= å…¬å…±é€»è¾‘ï¼šæ˜¾ç¤ºæ¥æº =================
            if retrieved_sources:
                with st.expander(f"ğŸ“š å‚è€ƒäº† {len(retrieved_sources)} ä¸ªåˆ‡ç‰‡ (Reranked)"):
                    for idx, src in enumerate(retrieved_sources):
                        # å°è¯•æå–åˆ†æ•° (éœ€ç¡®è®¤ API è¿”å›ç»“æ„ä¸­æ˜¯å¦åŒ…å«äº† scoreï¼Œå¯é€‰)
                        # å¦‚æœåç«¯ Source schema æ²¡æ”¹ï¼Œå¯èƒ½éœ€è¦é€šè¿‡ extra å­—æ®µä¼ é€’ï¼Œè¿™é‡Œæš‚æ—¶åšä¸ªå®¹é”™
                        score_display = ""
                        # if 'metadata' in src and 'rerank_score' in src['metadata']: ...
                        
                        page_num = src.get("page_number")
                        page_info = f" (P{page_num})" if page_num else ""
                        st.markdown(f"**[{idx+1}] {src['source_filename']}{page_info}**")
                        st.caption(src['chunk_content'])
            
            # æ›´æ–° Session State
            if full_response and full_response != "Error":
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response,
                    "sources": retrieved_sources
                })