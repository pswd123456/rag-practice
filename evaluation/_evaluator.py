# -*- coding: utf-8 -*-
"""
RAG è¯„ä¼°å™¨ (evaluator.py)

è´Ÿè´£:
1. åŠ è½½å’Œé¢„å¤„ç† Ragas æµ‹è¯•é›† (testset)ã€‚
2. ä½¿ç”¨ RAG ç®¡é“ä¸ºæµ‹è¯•é›†ç”Ÿæˆ 'answer' å’Œ 'contexts'ã€‚
3. è¿è¡Œ Ragas æŒ‡æ ‡ (Faithfulness, AnswerRelevancyç­‰) è¿›è¡Œè¯„ä¼°ã€‚
4. ä¿å­˜è¯„ä¼°åˆ†æ•°ã€‚
5. å¯é€šè¿‡ `python -m evaluation.evaluator` ç‹¬ç«‹è¿è¡Œã€‚
"""

# --- Ragas å’Œ Datasets å¯¼å…¥ ---
from datasets import load_dataset, Dataset
from ragas.metrics import (
    Faithfulness,
    AnswerRelevancy,
    ContextRecall,
    ContextPrecision
)
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas import evaluate

# --- é¡¹ç›®å†…éƒ¨å¯¼å…¥ ---
from app.core.config import settings
from app.services import ingest
from app.services.llm.llm_factory import setup_qwen_llm
from app.services.embedding.embedding_factory import setup_hf_embed_model
from app.services.pipeline import RAGPipeline

# --- æ—¥å¿—å’Œæ ‡å‡†åº“å¯¼å…¥ ---
import logging
import logging.config
import os
import sys
from app.core.logging_setup import get_logging_config
import warnings

warnings.filterwarnings(
    "ignore", 
    message=".*Torch was not compiled with flash attention.*"
)

# --- é…ç½®å…¨å±€æ—¥å¿— (ä»é…ç½®åŠ è½½) ---
# (ç¡®ä¿ 'logs' æ–‡ä»¶å¤¹å­˜åœ¨)
os.makedirs(settings.LOG_DIR, exist_ok=True) 
logging_config_dict = get_logging_config(str(settings.LOG_FILE_PATH))
logging.config.dictConfig(logging_config_dict)
# --- é…ç½®å®Œæˆ ---

# è·å– 'evaluator' æ¨¡å—çš„ logger
logger = logging.getLogger(__name__)


class RAGEvaluator:
    """
    å°è£…äº† RAG è¯„ä¼°æ‰€éœ€çš„æ‰€æœ‰é€»è¾‘ï¼ŒåŒ…æ‹¬:
    - æ•°æ®åŠ è½½å’Œå¤„ç† (load_and_process_testset)
    - RAG ç®¡é“é›†æˆ (_integrate_testset)
    - Ragas è¯„ä¼°æ‰§è¡Œ (run_evaluation)
    - ç»“æœä¿å­˜ (save_results)
    """
    def __init__(self, rag_pipeline: RAGPipeline,llm,embed_model):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨ã€‚

        å‚æ•°:
            rag_pipeline (RAGPipeline): ä¸€ä¸ªå·²å®ä¾‹åŒ–çš„ã€å‡†å¤‡å°±ç»ªçš„ RAG ç®¡é“å¯¹è±¡ã€‚
        """
        self.pipeline = rag_pipeline

        logger.debug("æ­£åœ¨åˆå§‹åŒ– Ragas LLM å’Œ Embeddings åŒ…è£…å™¨...")
        ragas_llm = LangchainLLMWrapper(llm)
        ragas_embed = LangchainEmbeddingsWrapper(embed_model)
        
        self.metrics = [
            Faithfulness(llm=ragas_llm),
            AnswerRelevancy(llm=ragas_llm, embeddings=ragas_embed),
            ContextRecall(llm=ragas_llm),
            ContextPrecision(llm=ragas_llm)     
        ]
        logger.info("Ragas è¯„ä¼°æŒ‡æ ‡å·²åˆå§‹åŒ–ã€‚")

        self.test_dataset = None
        self.scores_df = None

    def load_and_process_testset(self):
        """
        åŠ è½½åŸå§‹æµ‹è¯•é›† (CSV), å¹¶å°†å…¶å¤„ç†ä¸º Ragas è¯„ä¼°æ‰€éœ€çš„æ ¼å¼ã€‚
        
        æ­¥éª¤:
        1. ä» TESTSET_OUTPUT_PATH åŠ è½½ CSVã€‚
        2. é‡å‘½ååˆ— (e.g., 'user_input' -> 'question', 'reference' -> 'ground_truth')ã€‚
        3. åˆ é™¤è¯„ä¼°ä¸éœ€è¦çš„åŸå§‹åˆ—ã€‚
        4. é€šè¿‡ .map() è°ƒç”¨ RAG ç®¡é“ (_integrate_testset)ï¼Œ
           ä¸ºæ•°æ®é›†åŠ¨æ€ç”Ÿæˆ 'answer' å’Œ 'contexts' åˆ—ã€‚
        
        è¿”å›:
            datasets.Dataset: å¤„ç†å®Œæˆå¹¶åŒ…å« RAG è¾“å‡ºçš„æµ‹è¯•é›†ã€‚
        """
        logger.info("å¼€å§‹åŠ è½½å’Œå¤„ç†æµ‹è¯•é›†...")
        hf_dataset =  load_dataset("csv", data_files=str(settings.TESTSET_OUTPUT_PATH))

        self.test_dataset = hf_dataset["train"]
        assert isinstance(self.test_dataset, Dataset)    

        logger.debug("é‡å‘½ååˆ—ä»¥åŒ¹é… Ragas schema (question, ground_truth)...")
        rename_columns_dict = {
            "user_input": "question",
            "reference": "ground_truth",
        }
        self.test_dataset = self.test_dataset.rename_columns(rename_columns_dict)
        
        logger.debug("åˆ é™¤ä¸å¿…è¦çš„åŸå§‹åˆ—...")
        self.test_dataset = self.test_dataset.remove_columns(["reference_contexts", 'synthesizer_name'])
        
        logger.info("å¼€å§‹ä½¿ç”¨ RAG ç®¡é“ä¸ºæµ‹è¯•é›†ç”Ÿæˆ 'answer' å’Œ 'contexts' (map)...")
        self.test_dataset = self.test_dataset.map(
            self._integrate_testset, 
            batched=True, 
            batch_size=16  # (å¯ä»¥è°ƒæ•´æ‰¹æ¬¡å¤§å°)
        )
        logger.info("æµ‹è¯•é›†å¤„ç†å’Œ RAG ç®¡é“é›†æˆå®Œæˆã€‚")
        return self.test_dataset
    
    def _integrate_testset(self, batch):
        """
        (å†…éƒ¨è¾…åŠ©å‡½æ•°) ç”± .map() è°ƒç”¨ï¼Œç”¨äºæ‰¹é‡å¤„ç†æµ‹è¯•é›†ã€‚
        
        å‚æ•°:
            batch (dict): Hugging Face Datasets ä¼ é€’çš„æ‰¹å¤„ç†æ•°æ®ã€‚

        è¿”å›:
            dict: åŒ…å« 'answer' å’Œ 'contexts' åˆ—è¡¨çš„å­—å…¸ï¼Œå°†ä½œä¸ºæ–°åˆ—æ·»åŠ ã€‚
        """
        logger.debug(f"æ­£åœ¨å¤„ç†æ‰¹æ¬¡ï¼Œå¤§å°: {len(batch['question'])}")
        questions  = batch["question"]

        # 1. æ‰¹é‡æ£€ç´¢
        retriever = self.pipeline.get_retriever()
        contexts_docs = retriever.batch(questions)

        # (å°† Document åˆ—è¡¨è½¬æ¢ä¸º Ragas æœŸæœ›çš„ str åˆ—è¡¨)
        contexts_str_lists = [
            [doc.page_content for doc in doc_list] 
            for doc_list in contexts_docs
        ]

        # 2. æ‰¹é‡ç”Ÿæˆ
        generation_chain = self.pipeline.get_generation_chain()
        inputs_for_chain = []
        for q, c_list in zip(questions, contexts_str_lists):
            formatted_context = "\n\n".join(c_list)
            inputs_for_chain.append({
                "question": q,
                "context": formatted_context
            })

        answer_list = generation_chain.batch(inputs_for_chain)

        return {
            "answer": answer_list,
            "contexts": contexts_str_lists
        }

    def run_evaluation(self):
        """
        æ‰§è¡Œ Ragas è¯„ä¼°ã€‚
        
        å¦‚æœæµ‹è¯•é›†æœªåŠ è½½ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨ load_and_process_testset()ã€‚
        è¯„ä¼°ç»“æœ (å­—å…¸) ä¼šè¢«æ‰“å°ï¼Œå¹¶è½¬æ¢ä¸º Pandas DataFrame å­˜å‚¨åœ¨ self.scores_df ä¸­ã€‚
        
        è¿”å›:
            ragas.Result: Ragas è¯„ä¼°ç»“æœå¯¹è±¡ã€‚
        """
        logger.info("å¼€å§‹æ‰§è¡Œ Ragas è¯„ä¼°...")
        if self.test_dataset is None:
            logger.info("æµ‹è¯•é›† (self.test_dataset) æœªåŠ è½½ï¼Œè‡ªåŠ¨å¼€å§‹åŠ è½½å’Œå¤„ç†...")
            self.load_and_process_testset()

        result = evaluate(
            self.test_dataset, #type: ignore
            metrics=self.metrics     
        )

        logger.info(f"Ragas è¯„ä¼°ç»“æœ (å­—å…¸): {result}")

        self.scores_df = result.to_pandas()#type: ignore
        logger.info("è¯„ä¼°åˆ†æ•°å·²è½¬æ¢ä¸º Pandas DataFrameã€‚")
        return result
    
    def save_results(self):
        """
        å°†è¯„ä¼°ç»“æœ (Pandas DataFrame) ä¿å­˜åˆ° CSV æ–‡ä»¶ã€‚
        è·¯å¾„ç”± config.SCORE_CSV_PATH å®šä¹‰ã€‚
        """
        if self.scores_df is None:
            logger.warning("è¯„ä¼°åˆ†æ•° (scores_df) ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜ã€‚è¯·å…ˆè°ƒç”¨ run_evaluation()ã€‚")
            return
        
        output_csv_path = settings.EVALUATION_CSV_PATH
        try:
            self.scores_df.to_csv(output_csv_path, index=False) 
            logger.info(f"è¯„ä¼°ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {output_csv_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜è¯„ä¼°ç»“æœåˆ° {output_csv_path} å¤±è´¥: {e}", exc_info=True)


if __name__ == "__main__":
    """
    å½“æ–‡ä»¶ä½œä¸ºä¸»ç¨‹åº (python -m evaluation.evaluator) è¿è¡Œæ—¶çš„ä¸»å…¥å£ã€‚
    """
    logger.info("===================")
    logger.info("RAG è¯„ä¼°å™¨ (evaluator.py) å¯åŠ¨...")
    logger.info("===================")
    
    try:
        logger.info("ğŸ› ï¸ æ­£åœ¨è®¾ç½® RAG ç®¡é“...")
        collection_name = settings.CHROMADB_COLLECTION_NAME

        embeddings_name = "Qwen3-Embedding-0.6B"
        embeddings = setup_hf_embed_model(embeddings_name)
        logger.info(f" embeddings æ¨¡å‹å·²å°±ç»ª: {embeddings_name}") 

        vector_store = ingest.build_or_get_vector_store(collection_name,embeddings)
        
        llm_name = "qwen-flash"
        llm = setup_qwen_llm(llm_name)
        logger.info(f" LLM æ¨¡å‹å·²å°±ç»ª: {llm_name}")
        
        retriever = vector_store.as_retriever() 
        rag_pipeline = RAGPipeline(llm=llm, retriever=retriever)
        logger.info("RAG ç®¡é“å·²å°±ç»ªã€‚")

        logger.info(" evaluator...")
        evaluator = RAGEvaluator(rag_pipeline=rag_pipeline,llm=llm,embed_model=embeddings)
        
        logger.info("âš™ï¸ æ­£åœ¨åŠ è½½å’Œå¤„ç†æµ‹è¯•é›†...")
        # (æ³¨æ„: å³ä½¿ run_evaluation æœ‰æ£€æŸ¥ï¼Œè¿™é‡Œä¹Ÿä¿æŒåŸå§‹é€»è¾‘æ˜¾å¼è°ƒç”¨)
        evaluator.load_and_process_testset()
        
        logger.info("âš™ï¸ æ­£åœ¨è¿è¡Œè¯„ä¼°...")
        results = evaluator.run_evaluation()

        logger.info("âœ… è¯„ä¼°å®Œæˆ!")
        
        if evaluator.scores_df is not None:
            logger.info(f"è¯„ä¼°åˆ†æ•° (DataFrame æ‘˜è¦):\n{evaluator.scores_df.describe()}")
        else:
            logger.warning("è¯„ä¼°æœªç”Ÿæˆ scores_dfã€‚")

        evaluator.save_results()

    except Exception as e:
        logger.critical(f"è¯„ä¼°å™¨ä¸»ç¨‹åºè¿è¡Œå¤±è´¥: {e}", exc_info=True)
        sys.exit(1)

    logger.info("===================")
    logger.info("è¯„ä¼°å™¨ (evaluator.py) è¿è¡Œç»“æŸã€‚")
    logger.info("===================")