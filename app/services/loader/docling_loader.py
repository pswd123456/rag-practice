"""
app/services/loader/docling_loader.py
"""
import logging
import torch
import json
import os
from typing import List, Optional
from pathlib import Path

# LangChain Document
from langchain_core.documents import Document

# Docling
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.datamodel.accelerator_options import AcceleratorOptions, AcceleratorDevice

# Docling Chunking
from docling.chunking import HybridChunker
from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer
from transformers import AutoTokenizer

# Config
from app.core.config import settings, PROJECT_ROOT

logger = logging.getLogger(__name__)

class DoclingLoader:
    """
    åŸºäº Docling çš„æ–‡æ¡£åŠ è½½å™¨ï¼Œæ”¯æŒ PDF å’Œ Docxã€‚
    æ”¯æŒç›´æ¥å¯¼å‡º Markdown æˆ–ä½¿ç”¨ HybridChunker è¿›è¡Œè¯­ä¹‰åˆ‡ç‰‡ã€‚
    """
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self._converter = self._init_converter()

    def _init_converter(self) -> DocumentConverter:
        """
        åˆå§‹åŒ– Converterï¼Œé…ç½® GPU åŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        """
        # é…ç½® Pipeline é€‰é¡¹
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # å¼€å¯ OCR ä»¥å¤„ç†æ‰«æä»¶
        pipeline_options.do_table_structure = True # å¼€å¯è¡¨æ ¼ç»“æ„æå–

        # GPU åŠ é€Ÿé…ç½®
        if torch.cuda.is_available():
            # logger.info("ğŸš€ Docling æ£€æµ‹åˆ° CUDA ç¯å¢ƒï¼Œæ­£åœ¨å¯ç”¨ GPU åŠ é€Ÿ...")
            pipeline_options.accelerator_options = AcceleratorOptions(
                num_threads=4, 
                device=AcceleratorDevice.CUDA
            )
        else:
            logger.warning("âš ï¸ æœªæ£€æµ‹åˆ° CUDAï¼ŒDocling å°†ä½¿ç”¨ CPU è¿è¡Œ (é€Ÿåº¦è¾ƒæ…¢)")
            pipeline_options.accelerator_options = AcceleratorOptions(
                num_threads=8, 
                device=AcceleratorDevice.CPU
            )

        # ç»‘å®šæ ¼å¼é…ç½®
        return DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )

    def _save_debug_files(self, doc_content, markdown_text: str):
        """
        [Debug Logic] ä¿å­˜ä¸­é—´è§£æç»“æœåˆ°é¡¹ç›®æ ¹ç›®å½•
        """
        try:
            # 1. æ„é€ æ–‡ä»¶å
            original_stem = Path(self.file_path).stem
            # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦ä»¥å…è·¯å¾„æŠ¥é”™
            safe_stem = "".join([c for c in original_stem if c.isalnum() or c in (' ', '-', '_')]).strip()
            
            json_filename = f"debug_docling_{safe_stem}.json"
            md_filename = f"debug_docling_{safe_stem}.md"
            
            json_path = PROJECT_ROOT / json_filename
            md_path = PROJECT_ROOT / md_filename

            logger.info(f"ğŸ› [Debug] æ­£åœ¨ä¿å­˜ Docling ä¸­é—´æ–‡ä»¶åˆ°æ ¹ç›®å½•...")

            # 2. ä¿å­˜ Markdown å†…å®¹
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(markdown_text)
            
            # 3. å°è¯•ä¿å­˜ JSON
            if hasattr(doc_content, "export_to_dict"):
                doc_dict = doc_content.export_to_dict()
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(doc_dict, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"ğŸ› [Debug] ä¿å­˜è°ƒè¯•æ–‡ä»¶å¤±è´¥: {e}")

    def load(self) -> List[Document]:
        """
        (Legacy) åŠ è½½æ–‡æ¡£å¹¶è½¬æ¢ä¸ºå•ä¸€çš„ Markdown LangChain Documentã€‚
        é€‚ç”¨äºåç»­ä½¿ç”¨ RecursiveSplitter çš„åœºæ™¯ã€‚
        """
        return self._process_doc(chunking=False)

    def load_and_chunk(self, chunk_size: int = 512, chunk_overlap: int = 50) -> List[Document]:
        """
        [New] åŠ è½½å¹¶ä½¿ç”¨ HybridChunker è¿›è¡Œåˆ‡ç‰‡ã€‚
        
        :param chunk_size: Token é™åˆ¶ (HybridChunker ä½¿ç”¨ Tokenizer è®¡æ•°)
        :param chunk_overlap: è¿™é‡Œçš„ overlap HybridChunker ä¸ä¸€å®šå®Œå…¨éµå¾ªï¼Œå®ƒæœ‰è‡ªå·±çš„é€»è¾‘
        """
        return self._process_doc(chunking=True, max_tokens=chunk_size)

    def _process_doc(self, chunking: bool, max_tokens: int = 512) -> List[Document]:
        if not Path(self.file_path).exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {self.file_path}")

        logger.info(f"å¼€å§‹ä½¿ç”¨ Docling è§£ææ–‡ä»¶: {self.file_path} (Chunking={chunking})")
        
        try:
            # 1. æ ¸å¿ƒè½¬æ¢
            conversion_result = self._converter.convert(self.file_path)
            doc_content = conversion_result.document
            
            # Debug: å§‹ç»ˆä¿å­˜ Markdown ä»¥ä¾¿äººå·¥æ£€æŸ¥
            # try:
            #     markdown_text = doc_content.export_to_markdown()
            #     self._save_debug_files(doc_content, markdown_text)
            # except Exception:
            #     pass

            final_docs = []

            # 2. åˆ†æ”¯å¤„ç†
            if chunking:
                # === Hybrid Chunking é€»è¾‘ ===
                logger.info(f"åˆå§‹åŒ– HybridChunker (Tokenizer: {settings.CHUNK_TOKENIZER_ID}, MaxTokens: {max_tokens})")
                
                # åˆå§‹åŒ– Tokenizer (Lazily loaded usually, but here we init explicitly)
                # æ³¨æ„ï¼šAutoTokenizer éœ€è¦è”ç½‘ä¸‹è½½æ¨¡å‹é…ç½®ï¼ŒWorker ç¯å¢ƒéœ€ç¡®ä¿ç½‘ç»œæˆ–å·²ç¼“å­˜
                hf_tokenizer = AutoTokenizer.from_pretrained(settings.CHUNK_TOKENIZER_ID)
                tokenizer = HuggingFaceTokenizer(
                    tokenizer=hf_tokenizer, 
                    max_tokens=max_tokens # <--- è¿™é‡Œå¿…é¡»ä¼ ï¼Œé€šå¸¸æ˜¯ 512
                )
                
                chunker = HybridChunker(
                    tokenizer=tokenizer,
                    max_tokens=max_tokens,
                    merge_peers=True
                )
                
                chunk_iter = chunker.chunk(dl_doc=doc_content)
                
                for i, chunk in enumerate(chunk_iter):
                    # è·å–å¢å¼ºåçš„ä¸Šä¸‹æ–‡æ–‡æœ¬ (åŒ…å«æ ‡é¢˜å±‚çº§ç­‰)
                    enriched_text = chunker.contextualize(chunk=chunk)
                    
                    metadata = {
                        "source": str(self.file_path),
                        "filename": Path(self.file_path).name,
                        "chunk_index": i,
                        # å°è¯•ä» Docling å…ƒæ•°æ®ä¸­æå–é¡µç ç­‰ä¿¡æ¯ (å¯èƒ½åˆ†å¸ƒåœ¨ prov items ä¸­)
                        "doc_items": [str(item) for item in chunk.meta.doc_items] if hasattr(chunk.meta, "doc_items") else [],
                        "headings": chunk.meta.headings if hasattr(chunk.meta, "headings") else []
                    }
                    
                    final_docs.append(Document(page_content=enriched_text, metadata=metadata))
                
                logger.info(f"HybridChunker ç”Ÿæˆäº† {len(final_docs)} ä¸ªåˆ‡ç‰‡ã€‚")
                
            else:
                # === Legacy Logic: å…¨æ–‡ Markdown ===
                metadata = {
                    "source": str(self.file_path),
                    "filename": Path(self.file_path).name,
                    "page_count": len(doc_content.pages) if hasattr(doc_content, "pages") else 0,
                }
                final_docs = [Document(page_content=markdown_text, metadata=metadata)]

            return final_docs

        except Exception as e:
            logger.error(f"Docling è§£æ/åˆ‡ç‰‡å¤±è´¥: {e}", exc_info=True)
            raise e

# é€‚é…å‡½æ•° (Updated)
def load_and_chunk_docling_document(file_path: str, chunk_size: int = 512) -> List[Document]:
    loader = DoclingLoader(file_path)
    return loader.load_and_chunk(chunk_size=chunk_size)

def load_docling_document(file_path: str) -> List[Document]:
    loader = DoclingLoader(file_path)
    return loader.load()