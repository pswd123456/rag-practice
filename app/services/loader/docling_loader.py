"""
app/services/loader/docling_loader.py
"""
import logging
import torch
import json
import os
from typing import List, Optional
from pathlib import Path

# LangChain Document
from langchain_core.documents import Document

# Docling
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions, PictureDescriptionVlmOptions
from docling.datamodel.accelerator_options import AcceleratorOptions, AcceleratorDevice

# Docling Chunking
from docling.chunking import HybridChunker
from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer
from transformers import AutoTokenizer

# Config
from app.core.config import settings, PROJECT_ROOT

logger = logging.getLogger(__name__)

class DoclingLoader:
    """
    åŸºäºŽ Docling çš„æ–‡æ¡£åŠ è½½å™¨ï¼Œæ”¯æŒ PDF å’Œ Docxã€‚
    æ”¯æŒç›´æŽ¥å¯¼å‡º Markdown æˆ–ä½¿ç”¨ HybridChunker è¿›è¡Œè¯­ä¹‰åˆ‡ç‰‡ã€‚
    """
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self._converter = self._init_converter()

    def _init_converter(self) -> DocumentConverter:
        """
        åˆå§‹åŒ– Converterï¼Œé…ç½® GPU åŠ é€Ÿï¼ˆå¦‚æžœå¯ç”¨ï¼‰
        """

        local_models_path = settings.DOCLING_MODELS_PATH

        # é…ç½® Pipeline é€‰é¡¹
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # å¼€å¯ OCR ä»¥å¤„ç†æ‰«æä»¶
        pipeline_options.do_table_structure = True # å¼€å¯è¡¨æ ¼ç»“æž„æå–
        pipeline_options.do_formula_enrichment = True
        pipeline_options.do_picture_description = True 
        # å›¾ç‰‡æè¿°ç”¨çš„é»˜è®¤çš„256Mçš„SmolVLM, ä¸€è¨€éš¾å°½, ç”±äºŽæœºå™¨æ€§èƒ½é™åˆ¶åªèƒ½ç”¨è¿™ä¸ª, åŽç»­å¯ä»¥è‡ªå·±æ¢ä¸€ä¸‹
        pipeline_options.picture_description_options = PictureDescriptionVlmOptions(
            repo_id="HuggingFaceTB/SmolVLM-256M-Instruct",
            
            #"Describe this image in a few sentences."
            prompt="Briefly describe the main subject of this image. If it is a chart, explain what it shows."
        )

        pipeline_options.images_scale = 2.0
        pipeline_options.artifacts_path = local_models_path

        # GPU åŠ é€Ÿé…ç½®
        if torch.cuda.is_available():
            # logger.info("ðŸš€ Docling æ£€æµ‹åˆ° CUDA çŽ¯å¢ƒï¼Œæ­£åœ¨å¯ç”¨ GPU åŠ é€Ÿ...")
            pipeline_options.accelerator_options = AcceleratorOptions(
                num_threads=4, 
                device=AcceleratorDevice.CUDA
            )
        else:
            logger.warning("âš ï¸ æœªæ£€æµ‹åˆ° CUDAï¼ŒDocling å°†ä½¿ç”¨ CPU è¿è¡Œ (é€Ÿåº¦è¾ƒæ…¢)")
            pipeline_options.accelerator_options = AcceleratorOptions(
                num_threads=4, 
                device=AcceleratorDevice.CPU
            )

        # ç»‘å®šæ ¼å¼é…ç½®
        return DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )

    def load(self) -> List[Document]:
        """
        åŠ è½½æ–‡æ¡£å¹¶è½¬æ¢ä¸ºå•ä¸€çš„ Markdown LangChain Documentã€‚
        """
        return self._process_doc(chunking=False)

    def load_and_chunk(self, chunk_size: int = 512, chunk_overlap: int = 50) -> List[Document]:
        """
        åŠ è½½å¹¶ä½¿ç”¨ HybridChunker è¿›è¡Œåˆ‡ç‰‡ã€‚
        
        :param chunk_size: Token é™åˆ¶ (HybridChunker ä½¿ç”¨ Tokenizer è®¡æ•°)
        :param chunk_overlap: è¿™é‡Œçš„ overlap HybridChunker ä¸ä¸€å®šå®Œå…¨éµå¾ªï¼Œå®ƒæœ‰è‡ªå·±çš„é€»è¾‘
        """
        return self._process_doc(chunking=True, max_tokens=chunk_size)

    def _process_doc(self, chunking: bool, max_tokens: int = 512) -> List[Document]:
        if not Path(self.file_path).exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {self.file_path}")

        logger.info(f"å¼€å§‹ä½¿ç”¨ Docling è§£æžæ–‡ä»¶: {self.file_path} (Chunking={chunking})")
        
        try:
            # 1. æ ¸å¿ƒè½¬æ¢
            conversion_result = self._converter.convert(self.file_path)
            doc_content = conversion_result.document
            
            final_docs = []

            # 2. åˆ†æ”¯å¤„ç†
            if chunking:
                # === Hybrid Chunking é€»è¾‘ ===
                logger.info(f"åˆå§‹åŒ– HybridChunker (Tokenizer: {settings.CHUNK_TOKENIZER_ID}, MaxTokens: {max_tokens})")
                
                # åˆå§‹åŒ– Tokenizer
                hf_tokenizer = AutoTokenizer.from_pretrained(settings.CHUNK_TOKENIZER_ID)
                tokenizer = HuggingFaceTokenizer(
                    tokenizer=hf_tokenizer, 
                    max_tokens=max_tokens
                )
                
                chunker = HybridChunker(
                    tokenizer=tokenizer,
                    max_tokens=max_tokens,
                    merge_peers=True
                )
                
                chunk_iter = chunker.chunk(dl_doc=doc_content)
                
                for i, chunk in enumerate(chunk_iter):
                    # èŽ·å–å¢žå¼ºåŽçš„ä¸Šä¸‹æ–‡æ–‡æœ¬ (åŒ…å«æ ‡é¢˜å±‚çº§ç­‰)
                    enriched_text = chunker.contextualize(chunk=chunk)
                
                    page_numbers = set()
                    
                    # èŽ·å– doc_items
                    doc_items = getattr(chunk.meta, "doc_items", []) or []

                    for item in doc_items:
                        provs = []
                        if hasattr(item, "prov"):
                            provs = item.prov
                        elif isinstance(item, dict) and "prov" in item:
                            provs = item["prov"]
                        
                        if not provs:
                            continue
                        
                        for prov in provs:
                            p_no = None
                            if hasattr(prov, "page_no"):
                                p_no = prov.page_no
                            elif isinstance(prov, dict) and "page_no" in prov:
                                p_no = prov["page_no"]

                            if p_no is not None:
                                page_numbers.add(p_no)
                    # ---------------------------------

                    # æŽ’åºå¹¶ç”Ÿæˆæœ€ç»ˆåˆ—è¡¨
                    sorted_pages = sorted(list(page_numbers))
                    
                    metadata = {
                        "source": str(self.file_path),
                        "filename": Path(self.file_path).name,
                        "chunk_index": i,
                        "headings": chunk.meta.headings if hasattr(chunk.meta, "headings") else [],
                        "page_numbers": sorted_pages, # âœ… é¡µç ä¾ç„¶ä¿ç•™
                        "page_number": sorted_pages[0] if sorted_pages else None 
                    }
                    
                    final_docs.append(Document(page_content=enriched_text, metadata=metadata))
                
                logger.info(f"HybridChunker ç”Ÿæˆäº† {len(final_docs)} ä¸ªåˆ‡ç‰‡ã€‚")
                
            else:
                # === å…¨æ–‡ Markdown ===
                # è¿™æ®µç›®å‰åº”è¯¥æ˜¯ä¸å·¥ä½œçš„çŠ¶æ€
                markdown_text = doc_content.export_to_markdown()
                metadata = {
                    "source": str(self.file_path),
                    "filename": Path(self.file_path).name,
                    "page_count": len(doc_content.pages) if hasattr(doc_content, "pages") else 0,
                }
                final_docs = [Document(page_content=markdown_text, metadata=metadata)]

            return final_docs

        except Exception as e:
            logger.error(f"Docling è§£æž/åˆ‡ç‰‡å¤±è´¥: {e}", exc_info=True)
            raise e

# é€‚é…å‡½æ•°
def load_and_chunk_docling_document(file_path: str, chunk_size: int = 512) -> List[Document]:
    loader = DoclingLoader(file_path)
    return loader.load_and_chunk(chunk_size=chunk_size)

def load_docling_document(file_path: str) -> List[Document]:
    loader = DoclingLoader(file_path)
    return loader.load()