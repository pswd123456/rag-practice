"""
RAG ç®¡é“æ¨¡å— (pipeline.py)

è´Ÿè´£å®šä¹‰å’Œåˆ›å»º RAG (Retrieval-Augmented Generation) é“¾ã€‚
"""
import logging
from typing import AsyncGenerator, List, Optional, Union, Dict, Any

import tiktoken
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.messages import BaseMessage
from langfuse.langchain import CallbackHandler 
from langfuse import observe 

from app.services.generation.qa_service import QAService
from app.services.retrieval.retrieval_service import RetrievalService
from app.services.retrieval.vector_store_manager import VectorStoreManager
from app.services.factories.retrieval_factory import RetrievalFactory
from app.services.factories.llm_factory import setup_llm
from app.services.rerank.rerank_service import RerankService
from app.services.generation.rewrite_service import QueryRewriteService
# å¯¼å…¥ collapse_documents
from app.services.retrieval.fusion import collapse_documents

from app.core.config import settings

logger = logging.getLogger(__name__)


class RAGPipeline:
    def __init__(self, 
                 retrieval_service: RetrievalService, 
                 qa_service: QAService,
                 rerank_service: RerankService,
                 rewrite_service: QueryRewriteService

    ):
        """
        åˆå§‹åŒ– RAG ç®¡é“ï¼Œå°†æ£€ç´¢ä¸Žç”ŸæˆèŒè´£è§£è€¦ã€‚
        """
        logger.debug("åˆå§‹åŒ– RAGPipeline...")
        self.retrieval_service = retrieval_service
        self.qa_service = qa_service
        self.rerank_service = rerank_service
        self.langfuse_handler = CallbackHandler()
        self.generation_chain = self.qa_service.chain
        self.rewrite_service = rewrite_service
        
        # åˆå§‹åŒ– Tokenizer
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        self.rag_chain = (
            {
                "context": RunnableLambda(self.retrieval_service.afetch) | self._format_docs,
                "question": RunnablePassthrough(),
            }
            | self.generation_chain
        )

        logger.info("RAG ç®¡é“å·²æˆåŠŸåˆ›å»ºã€‚")

    @classmethod
    def build(
        cls,
        store_manager: VectorStoreManager,
        qa_service: QAService,
        rerank_service: RerankService, 
        knowledge_id: Optional[int] = None,
        recall_top_k: int = 50, 
        strategy: str = "hybrid", 
        **kwargs
    ) -> "RAGPipeline":
        """
        å·¥åŽ‚æ–¹æ³•ï¼šç»„è£… RAGPipeline
        """
        
        # å¼ºåˆ¶å…³é—­ Retriever å†…éƒ¨çš„æŠ˜å é€»è¾‘ï¼Œ
        # å°†æŠ˜å æ“ä½œå»¶è¿Ÿåˆ° Rerank ä¹‹åŽï¼Œåœ¨ Pipeline å±‚é¢å¤„ç†ã€‚
        retriever = RetrievalFactory.create_retriever(
            store_manager=store_manager,
            strategy=strategy,
            top_k=recall_top_k, 
            knowledge_id=knowledge_id,
            rerank_service=rerank_service,
            do_collapse=False, # ðŸŸ¢ Disable internal collapse
            **kwargs
        )

        rewrite_service = QueryRewriteService(llm=setup_llm("qwen-flash"))

        return cls(
            RetrievalService(retriever), 
            qa_service,
            rerank_service,
            rewrite_service
        )

    def _format_docs(self, docs: List[Document]) -> str:
        """
        æ™ºèƒ½æˆªæ–­ç­–ç•¥ (Token Aware)
        """
    
        max_total_tokens = settings.MAX_TOTAL_TOKENS    
        HIGH_QUALITY_THRESHOLD = 0.75 # Rerank åˆ†æ•°é˜ˆå€¼
        
        current_tokens = 0
        valid_docs = []
        
        logger.debug(f"Formatting {len(docs)} documents with Token-Aware Smart Truncation...")

        for doc in docs:
            # 1. èŽ·å–æˆ–è®¡ç®— Token æ•°
            if "token_count" in doc.metadata:
                doc_tokens = doc.metadata["token_count"]
            else:
                doc_tokens = len(self.tokenizer.encode(doc.page_content))
            
            score = doc.metadata.get("rerank_score", 0)
            
            # 2. é¢„åˆ¤æ˜¯å¦è¶…é™
            if current_tokens + doc_tokens > max_total_tokens:
                if score > HIGH_QUALITY_THRESHOLD and current_tokens < max_total_tokens * 0.9:
                    logger.info(f"ä¿ç•™é«˜åˆ†æ–‡æ¡£ (Score: {score:.3f}, Tokens: {doc_tokens})ï¼Œå°½ç®¡å³å°†è¶…é™ (Current: {current_tokens})ã€‚")
                    valid_docs.append(doc.page_content)
                    break 
                else:
                    logger.warning(f"è¾¾åˆ° Token ä¸Šé™ ({current_tokens}/{max_total_tokens})ï¼Œæˆªæ–­åŽç»­å†…å®¹ã€‚")
                    break
            
            valid_docs.append(doc.page_content)
            current_tokens += doc_tokens
            
        logger.info(f"Context ç»„è£…å®Œæˆ: {len(valid_docs)} docs, ~{current_tokens} tokens.")
        return "\n\n".join(valid_docs)

    async def _prepare_answer_async(self, inputs: Dict[str, Any], docs: List[Document]):
        """
        å¼‚æ­¥ç”Ÿæˆç­”æ¡ˆ
        """
        context = self._format_docs(docs)
        
        final_inputs = inputs.copy()
        final_inputs["context"] = context
        
        # æ³¨å…¥ Trace
        answer = await self.qa_service.ainvoke(
            final_inputs, 
            config={"callbacks": [self.langfuse_handler]}
        )
        return answer, docs

    @observe(name="rag_pipeline_run", as_type="chain")
    async def async_query(self, question: str, 
                          top_k: int = 3, 
                          threshold: float = None,
                          chat_history: List[BaseMessage] = None,
                          **kwargs):
        """
        å¼‚æ­¥å…¥å£ (New Flow: Recall(Child) -> Rerank(Child) -> Collapse(Parent) -> TopK -> Generate)
        """
        callbacks = {"callbacks": [self.langfuse_handler]}

        search_query = await self.rewrite_service.rewrite(
            question, chat_history or [], config=callbacks
        )

        # 1. Recall (è¿”å›ž Child Chunks)
        recall_child_docs = await self.retrieval_service.afetch(
            search_query, 
            config=callbacks
        )
        
        # 2. Rerank (Child Chunks)
        # Rerank æ‰€æœ‰çš„å€™é€‰ Childï¼Œç¡®ä¿é«˜ç›¸å…³æ€§çš„åˆ‡ç‰‡èƒ½æµ®ä¸Šæ¥
        reranked_child_docs = await self.rerank_service.rerank_documents(
            query=search_query,
            docs=recall_child_docs,
            top_n=len(recall_child_docs), # Rerank all retrieved docs
            threshold=threshold 
        )
        
        # 3. Collapse (Child -> Parent)
        # å°†æŽ’åºåŽçš„ Child æ˜ å°„å›ž Parentï¼Œå¹¶åŽ»é‡
        parent_docs = collapse_documents(reranked_child_docs)
        
        # 4. Top K Slice
        final_docs = parent_docs[:top_k]
        
        # 5. Generate
        inputs = {
            "question": question, 
            "chat_history": chat_history, 
            **kwargs
        }

        return await self._prepare_answer_async(inputs, final_docs)

    async def astream_with_sources(self, 
                                   query: str, 
                                   top_k: int = 3, 
                                   threshold: float = None, 
                                   chat_history: List[BaseMessage] = None,
                                   **kwargs) -> AsyncGenerator[Union[List[Document], str], None]:
        """
        æµå¼ç”Ÿæˆ (æ”¯æŒ Small-to-Big Rerank)
        """

        callbacks = {"callbacks": [self.langfuse_handler]}

        search_query = await self.rewrite_service.rewrite(
            query, chat_history or [], config=callbacks
        )

        # 1. Recall (Child)
        recall_child_docs = await self.retrieval_service.afetch(
            search_query, 
            config=callbacks
        )

        # 2. Rerank (Child)
        reranked_child_docs = await self.rerank_service.rerank_documents(
            query=search_query, 
            docs=recall_child_docs,
            top_n=len(recall_child_docs), # Rerank all
            threshold=threshold
        )
        
        # 3. Collapse (Child -> Parent)
        parent_docs = collapse_documents(reranked_child_docs)
        
        # 4. Top K Slice
        final_docs = parent_docs[:top_k]
        
        # å‘é€å¼•ç”¨æº
        yield final_docs
        
        # 5. Generate
        context = self._format_docs(final_docs)
        inputs = {
            "question": query, 
            "context": context, 
            "chat_history": chat_history, 
            **kwargs
        }
        
        async for chunk in self.generation_chain.astream(
        inputs,
        config={"callbacks": [self.langfuse_handler]}
        ):
            if chunk.content:
                yield chunk.content
        
            if chunk.usage_metadata:
                yield {"token_usage_payload": chunk.usage_metadata}
    
    def get_retrieval_service(self) -> RetrievalService:
        return self.retrieval_service

    def get_generation_chain(self):
        return self.generation_chain