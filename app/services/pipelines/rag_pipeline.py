# -*- coding: utf-8 -*-
"""
RAG ç®¡é“æ¨¡å— (pipeline.py)

è´Ÿè´£å®šä¹‰å’Œåˆ›å»º RAG (Retrieval-Augmented Generation) é“¾ã€‚
"""
import logging
from typing import AsyncGenerator, List, Optional, Union

from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langfuse.langchain import CallbackHandler # ğŸŸ¢ å¼•å…¥ Handler

from app.services.generation.qa_service import QAService
from app.services.retrieval.service import RetrievalService
from app.services.retrieval.vector_store_manager import VectorStoreManager

logger = logging.getLogger(__name__)


class RAGPipeline:
    def __init__(self, retrieval_service: RetrievalService, qa_service: QAService):
        """
        åˆå§‹åŒ– RAG ç®¡é“ï¼Œå°†æ£€ç´¢ä¸ç”ŸæˆèŒè´£è§£è€¦ã€‚
        """
        logger.debug("åˆå§‹åŒ– RAGPipeline...")
        self.retrieval_service = retrieval_service
        self.qa_service = qa_service
        
        # ğŸŸ¢ 1. åˆå§‹åŒ– Langfuse Callback
        # å®ƒä¼šè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å– LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY
        self.langfuse_handler = CallbackHandler()

        # æ„å»º LCEL é“¾ (ä¿æŒåŸé€»è¾‘ç”¨äºå¤‡ç”¨æˆ– sync query)
        self.generation_chain = self.qa_service.chain
        
        retrieval_node = RunnableLambda(
            func = self.retrieval_service.fetch,
            afunc = self.retrieval_service.afetch
        )
        
        self.rag_chain = (
            {
                "context": retrieval_node | self._format_docs,
                "question": RunnablePassthrough(),
            }
            | self.generation_chain
        )

        logger.info("RAG ç®¡é“å·²æˆåŠŸåˆ›å»ºã€‚")

    @classmethod
    def build(
        cls,
        store_manager:VectorStoreManager,
        qa_service: QAService,
        knowledge_id: Optional[int] = None,
        top_k: int = 3,
        strategy: str = "default"
    ) -> "RAGPipeline":
        """
        å·¥å‚æ–¹æ³•ï¼šæ ¹æ®å‚æ•°åŠ¨æ€ç»„è£… RAGPipeline
        """
        search_kwargs = {"k": top_k}
        if knowledge_id:
            search_kwargs["filter"] = {"knowledge_id": knowledge_id}#type:ignore

        if strategy == "dense_only":
            retriever = store_manager.vector_store.as_retriever(search_kwargs=search_kwargs)
                    
        elif strategy == "hybrid":
            # TODO: å®ç°æ··åˆæ£€ç´¢é€»è¾‘
            # retriever = ...
            retriever = store_manager.vector_store.as_retriever(search_kwargs=search_kwargs)
            
        elif strategy == "rerank":
            # TODO: å®ç°é‡æ’åºé€»è¾‘
            # retriever = ...
            retriever = store_manager.vector_store.as_retriever(search_kwargs=search_kwargs)
            
        else:
            # default
            retriever = store_manager.vector_store.as_retriever(search_kwargs=search_kwargs)

        return cls(RetrievalService(retriever), qa_service)

    def _format_docs(self, docs: List[Document]) -> str:
        logger.debug("æ­£åœ¨æ ¼å¼åŒ– %s ä¸ªæ£€ç´¢åˆ°çš„æ–‡æ¡£...", len(docs))
        formatted = "\n\n".join(doc.page_content for doc in docs)
        logger.debug("æ ¼å¼åŒ–åçš„ä¸Šä¸‹æ–‡é•¿åº¦: %s å­—ç¬¦", len(formatted))
        return formatted

    def _prepare_answer(self, question: str, docs: List[Document]):
        """åŒæ­¥ç”Ÿæˆç­”æ¡ˆï¼Œè¿”å›ç­”æ¡ˆå’Œæ–‡æ¡£"""
        context = self._format_docs(docs)
        answer = self.qa_service.invoke(question, context)
        return answer, docs

    async def _prepare_answer_async(self, question: str, docs: List[Document]):
        """å¼‚æ­¥ç”Ÿæˆç­”æ¡ˆï¼Œæ³¨å…¥ Tracing"""
        context = self._format_docs(docs)
        
        # ğŸŸ¢ 2. æ³¨å…¥ Callback åˆ°ç”Ÿæˆç¯èŠ‚
        # è¿™ä¼šè‡ªåŠ¨è®°å½• Generation Span (åŒ…æ‹¬ Promptã€Completionã€Token Usage)
        answer = await self.qa_service.ainvoke(
            question, 
            context, 
            config={"callbacks": [self.langfuse_handler]}
        )
        return answer, docs

    def query(self, question: str):
        docs = self.retrieval_service.fetch(question)
        return self._prepare_answer(question, docs)

    async def async_query(self, question: str):
        """æ‰§è¡Œå®Œæ•´ RAG æµç¨‹ (æ£€ç´¢ + ç”Ÿæˆ)"""
        # ğŸŸ¢ 3. æ³¨å…¥ Callback åˆ°æ£€ç´¢ç¯èŠ‚
        # è¿™ä¼šè‡ªåŠ¨è®°å½• Retrieval Span (åŒ…æ‹¬æŸ¥è¯¢è¯ã€å¬å›æ–‡æ¡£å†…å®¹)
        docs = await self.retrieval_service.afetch(
            question, 
            config={"callbacks": [self.langfuse_handler]}
        )
        
        return await self._prepare_answer_async(question, docs)
    
    async def astream_answer(self, query: str):
        async for chunk in self.rag_chain.astream(query):
            yield chunk

    async def astream_with_sources(self, query: str) -> AsyncGenerator[Union[List[Document], str], None]:
        """
        æµå¼ç”Ÿæˆï¼šå…ˆ Yield æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨ï¼Œå† Yield ç”Ÿæˆçš„ Tokenã€‚
        """
        # ğŸŸ¢ 4. æ£€ç´¢ Tracing
        docs = await self.retrieval_service.afetch(
            query,
            config={"callbacks": [self.langfuse_handler]}
        )
        yield docs
        
        context = self._format_docs(docs)
        payload = {"question": query, "context": context}
        
        # ğŸŸ¢ 5. ç”Ÿæˆ Tracing (æµå¼)
        # Langfuse ä¼šè‡ªåŠ¨èšåˆæµå¼å—ï¼Œåœ¨ Trace ä¸­æ˜¾ç¤ºå®Œæ•´å›å¤
        async for token in self.generation_chain.astream(
            payload,
            config={"callbacks": [self.langfuse_handler]}
        ):
            yield token
    def get_retrieval_service(self) -> RetrievalService:
        return self.retrieval_service

    def get_generation_chain(self):
        return self.generation_chain