"""
app/services/ingest/ingest.py
"""
import logging
import os
import uuid
import asyncio
import tempfile
from pathlib import Path
from sqlmodel import select

import tiktoken
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document as LangChainDocument

# ğŸŸ¢ å¼•å…¥ ES å¼‚å¸¸ä»¥ä¾¿æ•è·
from elasticsearch.helpers import BulkIndexError 

from app.db.session import async_session_maker
from sqlalchemy.orm import selectinload

from app.core.config import settings
from app.domain.models import Document, DocStatus, Knowledge
from app.services.loader.docling_loader import load_and_chunk_docling_document
from app.services.loader import load_single_document, split_docs
from app.services.factories import setup_embed_model
from app.services.retrieval.vector_store_manager import VectorStoreManager
from app.services.minio.file_storage import get_minio_client

logger = logging.getLogger(__name__)

async def process_document_pipeline(doc_id: int):
    """
    æ ¸å¿ƒæ–‡æ¡£å¤„ç†ç®¡é“
    Phases:
    1. DB: è·å–å…ƒæ•°æ®, çŠ¶æ€ -> PROCESSING
    2. No-DB: ä¸‹è½½, è§£æ(Docling/Basic), å‘é‡åŒ–
    3. DB: çŠ¶æ€ -> COMPLETED / FAILED
    """
    
    # -----------------------------------------------------
    # Phase 1: åˆå§‹åŒ–ä¸çŠ¶æ€æ›´æ–° (Short DB Transaction)
    # -----------------------------------------------------
    doc_filename = None
    doc_file_path = None
    doc_kb_id = None
    kb_id = None
    kb_chunk_size = None
    kb_chunk_overlap = None
    kb_embed_model = None
    kb_name = None

    async with async_session_maker() as db:
        # 1. è·å–æ–‡æ¡£å¹¶é¢„åŠ è½½å…³è”çš„ Knowledge
        stmt = select(Document).where(Document.id == doc_id).options(selectinload(Document.knowledge_base))
        result = await db.exec(stmt)
        doc = result.first()

        if not doc:
            logger.error(f"æ–‡æ¡£ {doc_id} ä¸å­˜åœ¨")
            return

        knowledge = doc.knowledge_base
        if not knowledge:
            knowledge = await db.get(Knowledge, doc.knowledge_base_id)
            if not knowledge:
                logger.error(f"å…³è”çš„çŸ¥è¯†åº“ {doc.knowledge_base_id} ä¸å­˜åœ¨")
                doc.status = DocStatus.FAILED
                doc.error_message = "å…³è”çš„çŸ¥è¯†åº“ä¸å­˜åœ¨"
                db.add(doc)
                await db.commit()
                return

        # æå–å¿…è¦æ•°æ®
        doc_filename = doc.filename
        doc_file_path = doc.file_path
        doc_kb_id = doc.knowledge_base_id
        
        kb_id = knowledge.id
        kb_name = knowledge.name
        kb_chunk_size = knowledge.chunk_size
        kb_chunk_overlap = knowledge.chunk_overlap
        kb_embed_model = knowledge.embed_model

        logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£ {doc_id} | KB: {kb_name} | File: {doc_filename}")

        # æ›´æ–°çŠ¶æ€
        doc.status = DocStatus.PROCESSING
        doc.error_message = None
        db.add(doc)
        await db.commit()
    
    # -----------------------------------------------------
    # Phase 2: æ ¸å¿ƒå¤„ç† (No DB Connection)
    # -----------------------------------------------------
    temp_file_path = None
    final_docs_to_ingest = []
    
    try:
        # 1. ä¸‹è½½æ–‡ä»¶
        minio_client = get_minio_client()
        original_suffix = Path(doc_filename).suffix.lower()
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=original_suffix) as tmp_file:
            temp_file_path = tmp_file.name
        
        def _download_task():
            minio_client.fget_object(
                bucket_name=settings.MINIO_BUCKET_NAME,
                object_name=doc_file_path,
                file_path=temp_file_path
            )
        await asyncio.to_thread(_download_task)
        
        # 2. åŠ è½½ä¸åˆ‡åˆ† (Updated for Parent-Child Indexing & Token Counting)
        def _load_and_split_task():
            # åˆå§‹åŒ– Tokenizer (cl100k_base é€‚ç”¨äº GPT-4, Qwen, DeepSeek ç­‰)
            try:
                tokenizer = tiktoken.get_encoding("cl100k_base")
            except Exception:
                # Fallback if specific encoding fails
                tokenizer = tiktoken.get_encoding("cl100k_base")

            # å®šä¹‰å­æ–‡æ¡£åˆ‡åˆ†å™¨ (Small Chunk)
            # Parent <- kb_chunk_size
            parent_chunk_size = kb_chunk_size
            child_chunk_size = 200
            child_overlap = 35
            
            parent_docs = []

            # A. ç”Ÿæˆ Parent Docs
            if original_suffix in [".pdf", ".docx", ".doc"]:
                logger.info(f"ä½¿ç”¨ Docling è§£æ Parent Docs (Size={parent_chunk_size})...")
                # ä½¿ç”¨ Docling ç”Ÿæˆè¾ƒå¤§çš„ Parent Chunks
                parent_docs = load_and_chunk_docling_document(temp_file_path, chunk_size=parent_chunk_size)
            else:
                logger.info(f"ä½¿ç”¨ BasicLoader è§£æ Parent Docs...")
                # æ™®é€šæ–‡ä»¶åŠ è½½
                raw_docs = load_single_document(temp_file_path)
                # åˆ‡åˆ†å‡º Parent
                parent_docs = split_docs(raw_docs, parent_chunk_size, kb_chunk_overlap)

            # B. ç”Ÿæˆ Child Docs å¹¶å…³è”
            logger.info(f"ç”Ÿæˆ Child Docs (Size={child_chunk_size}) å¹¶å»ºç«‹çˆ¶å­å…³è”...")
            
            child_splitter = RecursiveCharacterTextSplitter(
                chunk_size=child_chunk_size,
                chunk_overlap=child_overlap,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
            )
            
            results = []
            for p_doc in parent_docs:
                parent_id = str(uuid.uuid4())
                parent_content = p_doc.page_content
                
                # è®¡ç®— Parent Token æ•°å¹¶å­˜å…¥ Metadata
                token_count = len(tokenizer.encode(parent_content))
                
                # åˆ‡åˆ† Child
                child_chunks = child_splitter.split_documents([p_doc])
                
                for c_doc in child_chunks:
                    # ç»§æ‰¿å…ƒæ•°æ®
                    c_doc.metadata.update(p_doc.metadata)
                    
                    # æ³¨å…¥å…³é”®å…³è”ä¿¡æ¯
                    c_doc.metadata["doc_id"] = str(uuid.uuid4()) # Child Unique ID
                    c_doc.metadata["parent_id"] = parent_id      # Link to Parent
                    c_doc.metadata["parent_content"] = parent_content # Store Parent Content
                    c_doc.metadata["token_count"] = token_count  # Pre-calculated Tokens
                    
                    # è¡¥å……ä¸šåŠ¡å…ƒæ•°æ®
                    c_doc.metadata["source"] = str(doc_filename) # ğŸŸ¢ å¼ºåˆ¶ str
                    c_doc.metadata["knowledge_id"] = str(doc_kb_id) # ğŸŸ¢ å¼ºåˆ¶ str (å¯¹åº” keyword mapping)
                    
                    # å…¼å®¹ pyPDF / Docling é¡µç 
                    if "page" in c_doc.metadata and "page_number" not in c_doc.metadata:
                        c_doc.metadata["page_number"] = c_doc.metadata["page"]
                    
                    # ğŸŸ¢ æ¸…ç†/è§„èŒƒåŒ– page_number ç±»å‹ (ES å¯èƒ½ä¼šçº ç»“äº int vs None vs string)
                    if "page_number" in c_doc.metadata:
                        val = c_doc.metadata["page_number"]
                        if val is not None:
                            try:
                                c_doc.metadata["page_number"] = int(val)
                            except:
                                c_doc.metadata["page_number"] = 0

                    results.append(c_doc)
            
            return results

        final_docs_to_ingest = await asyncio.to_thread(_load_and_split_task)
        
        logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆã€‚Parents: N/A -> Children: {len(final_docs_to_ingest)}")

        # 4. å‘é‡åŒ–ä¸å…¥åº“ ES
        collection_name = f"kb_{kb_id}"
        
        def _vector_store_task():
            embed_model = setup_embed_model(kb_embed_model)
            manager = VectorStoreManager(collection_name, embed_model)
            manager.ensure_index()
            vector_store = manager.get_vector_store()
            
            logger.info(f"æ­£åœ¨å‘ ES ç´¢å¼• {manager.index_name} å†™å…¥åˆ‡ç‰‡...")
            # æ³¨æ„ï¼šES mapping å·²ç»é…ç½®äº† parent_content index: False
            return vector_store.add_documents(final_docs_to_ingest)

        await asyncio.to_thread(_vector_store_task)

        # -----------------------------------------------------
        # Phase 3: å®ŒæˆçŠ¶æ€æ›´æ–°
        # -----------------------------------------------------
        async with async_session_maker() as db:
            doc = await db.get(Document, doc_id)
            if doc:
                doc.status = DocStatus.COMPLETED
                db.add(doc)
                await db.commit()
                logger.info(f"æ–‡æ¡£ {doc_id} çŠ¶æ€å·²æ›´æ–°ä¸º COMPLETED")

    except BulkIndexError as e:
        # ğŸŸ¢ [Error Handling] ä¸“é—¨å¤„ç† ES Bulk å†™å…¥é”™è¯¯
        logger.error(f"ES Bulk å†™å…¥å¤±è´¥: {e}")
        
        err_msg = str(e)
        # æå–ç¬¬ä¸€ä¸ªé”™è¯¯åŸå› è¿›è¡Œç®€è¦åˆ†æ
        if e.errors:
            first_err = e.errors[0]
            # å¸¸è§æ ¼å¼: {'index': {'_index': '...', 'status': 400, 'error': {'type': 'mapper_parsing_exception', ...}}}
            if isinstance(first_err, dict):
                # å°è¯•æå– deep error info
                error_detail = first_err.get("index", {}).get("error", {})
                if isinstance(error_detail, dict):
                    err_type = error_detail.get("type", "")
                    err_reason = error_detail.get("reason", "")
                    if "mapper_parsing_exception" in err_type:
                        err_msg = f"ç´¢å¼•å­—æ®µç±»å‹å†²çª (Mapping Error)ã€‚è¯·å°è¯•åˆ é™¤æ—§ç´¢å¼• '{collection_name}' åé‡è¯•ã€‚Details: {err_reason}"
                    else:
                        err_msg = f"ES å†™å…¥é”™è¯¯: {err_reason}"
        
        logger.error(f"æ–‡æ¡£ {doc_id} å¤„ç†ä¸­æ–­ã€‚Root Cause: {err_msg}")
        
        async with async_session_maker() as db:
            doc = await db.get(Document, doc_id)
            if doc:
                doc.status = DocStatus.FAILED
                doc.error_message = err_msg[:1000] # é˜²æ­¢è¿‡é•¿
                db.add(doc)
                await db.commit()

    except Exception as e:
        logger.error(f"æ–‡æ¡£ {doc_id} å¤„ç†å¤±è´¥: {e}", exc_info=True)
        async with async_session_maker() as db:
            doc = await db.get(Document, doc_id)
            if doc:
                doc.status = DocStatus.FAILED
                doc.error_message = str(e)[:500]
                db.add(doc)
                await db.commit()

    finally:
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
            except Exception:
                pass