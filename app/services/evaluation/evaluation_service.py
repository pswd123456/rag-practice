import asyncio
import logging
import nest_asyncio
import tempfile
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any

from sqlmodel import select
from sqlmodel.ext.asyncio.session import AsyncSession
from sqlalchemy.orm import selectinload
from langfuse import Langfuse

# å¤ç”¨å·²æœ‰çš„ Ragas é€»è¾‘
from ragas.testset import TestsetGenerator
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.llms import LangchainLLMWrapper

from app.core.config import settings
from app.domain.models import Testset, Experiment, Document, Knowledge
from app.services.factories import setup_embed_model, setup_llm
from app.services.minio.file_storage import save_bytes_to_minio, get_minio_client

# [Modified] å¼•å…¥ä¸ Ingest ç®¡é“ä¸€è‡´çš„åŠ è½½å™¨
from app.services.loader.docling_loader import load_and_chunk_docling_document
from app.services.loader import load_single_document, split_docs

from app.services.retrieval import VectorStoreManager
from app.services.pipelines import RAGPipeline
from app.services.generation import QAService
from app.services.evaluation.evaluation_runner import RAGEvaluator

logger = logging.getLogger(__name__)

nest_asyncio.apply()

# ==========================================
# 1. æµ‹è¯•é›†ç”Ÿæˆ (Generate Testset)
# ==========================================

async def generate_testset_pipeline(db: AsyncSession, testset_id: int, source_doc_ids: List[int], generator_model: str = "qwen-max"):
    """
    æ ¹æ®æŒ‡å®šçš„æºæ–‡æ¡£ç”Ÿæˆæµ‹è¯•é›† (å¼‚æ­¥ç‰ˆ)
    [Consistency Fix]: ç¡®ä¿ä½¿ç”¨ä¸ Ingestion é˜¶æ®µå®Œå…¨ä¸€è‡´çš„åŠ è½½ä¸åˆ‡ç‰‡ç­–ç•¥ (Docling + HybridChunker)
    """
    testset = await db.get(Testset, testset_id)
    if not testset:
        logger.error(f"Testset {testset_id} not found")
        return

    try:
        logger.info(f"å¼€å§‹ä¸º Testset {testset_id} ç”Ÿæˆæ•°æ®ï¼Œæºæ–‡æ¡£ID: {source_doc_ids}")
        testset.status = "GENERATING"
        db.add(testset)
        await db.commit()
        
        # 2. é¢„åŠ è½½æ–‡æ¡£ä¿¡æ¯ (å« Knowledge é…ç½®)
        # [Fix] æˆ‘ä»¬éœ€è¦è·å– chunk_sizeï¼Œä»¥ä¾¿ Docling åˆ‡ç‰‡ä¸å…¥åº“æ—¶ä¸€è‡´
        doc_infos = []
        for doc_id in source_doc_ids:
            # æ˜¾å¼åŠ è½½ knowledge_base å…³ç³»
            stmt = select(Document).where(Document.id == doc_id).options(selectinload(Document.knowledge_base))
            result = await db.exec(stmt)
            db_doc = result.first()
            
            if db_doc:
                kb = db_doc.knowledge_base
                # é»˜è®¤å€¼é˜²å®ˆ
                c_size = kb.chunk_size if kb else settings.CHUNK_SIZE
                c_overlap = kb.chunk_overlap if kb else settings.CHUNK_OVERLAP
                
                doc_infos.append({
                    "filename": db_doc.filename,
                    "file_path": db_doc.file_path,
                    "chunk_size": c_size,
                    "chunk_overlap": c_overlap
                })
        
        if not doc_infos:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£è®°å½•")

        # å®šä¹‰é˜»å¡çš„åŠ è½½å‡½æ•° (Update)
        def _blocking_load(infos: List[Dict[str, Any]]):
            loaded_docs = []
            minio_client = get_minio_client()
            
            for info in infos:
                filename = info["filename"]
                file_path = info["file_path"]
                chunk_size = info["chunk_size"]
                chunk_overlap = info["chunk_overlap"]
                
                suffix = Path(filename).suffix.lower()
                
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                with tempfile.NamedTemporaryFile(delete=True, suffix=suffix) as tmp:
                    # ä¸‹è½½
                    minio_client.fget_object(
                        bucket_name=settings.MINIO_BUCKET_NAME, 
                        object_name=file_path, 
                        file_path=tmp.name
                    )
                    
                    # [Critical Fix] åˆ†æ”¯å¤„ç†ï¼šä¿æŒä¸ Ingest Pipeline ä¸€è‡´
                    if suffix in [".pdf", ".docx", ".doc"]:
                        logger.info(f"Testset Generation: ä½¿ç”¨ Docling å¤„ç† {filename} (Size={chunk_size})")
                        # ä½¿ç”¨ Docling HybridChunker
                        docs = load_and_chunk_docling_document(tmp.name, chunk_size=chunk_size)
                        loaded_docs.extend(docs)
                    else:
                        logger.info(f"Testset Generation: ä½¿ç”¨ BasicLoader å¤„ç† {filename}")
                        # ä½¿ç”¨æ ‡å‡†åŠ è½½ + RecursiveSplitter
                        raw_docs = load_single_document(tmp.name)
                        splitted = split_docs(raw_docs, chunk_size, chunk_overlap)
                        loaded_docs.extend(splitted)
                        
            return loaded_docs

        # åœ¨çº¿ç¨‹ä¸­æ‰§è¡ŒåŠ è½½
        langchain_docs = await asyncio.to_thread(_blocking_load, doc_infos)
        
        if not langchain_docs:
            raise ValueError("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æ¡£å†…å®¹")
        
        logger.info(f"æ–‡æ¡£åŠ è½½å®Œæˆï¼Œå…± {len(langchain_docs)} ä¸ªåˆ‡ç‰‡ã€‚å¼€å§‹ç”Ÿæˆ QA å¯¹...")

        # 3. æ‰§è¡Œç”Ÿæˆ (Ragas Generator)
        def _generation_task():
            generator_llm = setup_llm(generator_model) 
            # ä½¿ç”¨æ›´å¼ºçš„ Embedding æ¨¡å‹ä»¥æé«˜ Testset è´¨é‡ (Evaluation é€šå¸¸ä¸æƒœæˆæœ¬)
            generator_embed = setup_embed_model("text-embedding-v4")
            
            generator = TestsetGenerator(
                llm=LangchainLLMWrapper(generator_llm), 
                embedding_model=LangchainEmbeddingsWrapper(generator_embed)
            )
            
            # generate_with_langchain_docs ä¼šä½¿ç”¨ä¼ å…¥çš„ chunks (Nodes) ç”Ÿæˆé—®é¢˜
            # å› ä¸ºæˆ‘ä»¬ä¼ å…¥çš„æ˜¯ Docling åˆ‡å¥½çš„ chunksï¼Œæ‰€ä»¥ç”Ÿæˆçš„ Context ä¹Ÿæ˜¯åŸºäº Docling çš„
            return generator.generate_with_langchain_docs(
                langchain_docs, 
                testset_size=settings.TESTSET_SIZE
            )

        dataset = await asyncio.to_thread(_generation_task)
        
        # 4. è½¬ CSV å¹¶ä¿å­˜åˆ° MinIO (Thread)
        def _save_task():
            df = dataset.to_pandas() # type: ignore
            json_str = df.to_json(orient="records", lines=True, force_ascii=False)
            json_bytes = json_str.encode('utf-8')
            
            file_path = f"testsets/{testset_id}_{testset.name}.jsonl"
            save_bytes_to_minio(json_bytes, file_path, "application/json")
            
            # åŒæ­¥åˆ° Langfuse
            try:
                langfuse = Langfuse()
                lf_dataset_name = f"testset_{testset_id}_{testset.name}"
                langfuse.create_dataset(
                    name=lf_dataset_name,
                    description=f"Docs: {source_doc_ids}. Model: {generator_model}. (Docling/Aligned)",
                    metadata={"testset_id": testset_id, "source": "rag-practice"}
                )
                for _, row in df.iterrows():
                    langfuse.create_dataset_item(
                        dataset_name=lf_dataset_name,
                        input=row["user_input"],
                        expected_output=row["reference"],
                        metadata={"source_context": row.get("reference_contexts")}
                    )
            except Exception as e:
                logger.warning(f"Langfuse dataset upload failed: {e}")

            return file_path, len(df)

        saved_path, count = await asyncio.to_thread(_save_task)

        # 5. æ›´æ–° DB
        testset = await db.get(Testset, testset_id)
        if testset:
            testset.file_path = saved_path
            testset.description = f"Generated by {generator_model} (Docling Enabled). Size: {count}"
            testset.status = "COMPLETED"
            testset.error_message = None
            db.add(testset)
            await db.commit()
        
        logger.info(f"Testset {testset_id} ç”Ÿæˆå®Œæˆ")

    except Exception as e:
        logger.error(f"Testset ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
        await db.rollback()
        testset = await db.get(Testset, testset_id) 
        if testset:
            testset.status = "FAILED"
            testset.error_message = str(e)
            db.add(testset)
            await db.commit()
        raise e

# ... (Run Experiment éƒ¨åˆ†ä¿æŒä¸å˜) ...
async def run_experiment_pipeline(db: AsyncSession, experiment_id: int):
    # å¤ç”¨ä¹‹å‰çš„é€»è¾‘ï¼Œæœªä¿®æ”¹éƒ¨åˆ†çœç•¥ä»¥èŠ‚çœç¯‡å¹…ï¼Œä½†è¯·ä¿ç•™åŸæ–‡ä»¶ä¸­çš„å®Œæ•´ä»£ç 
    stmt = select(Experiment).where(Experiment.id == experiment_id).options(
        selectinload(Experiment.knowledge),
        selectinload(Experiment.testset)
    )
    result = await db.exec(stmt)
    exp = result.first()
    
    if not exp:
        return

    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œå®éªŒ {experiment_id}...")
        exp.status = "RUNNING"
        db.add(exp)
        await db.commit()

        # 2. å‡†å¤‡ç»„ä»¶
        kb = exp.knowledge
        ts = exp.testset
        dataset_name = f"testset_{ts.id}_{ts.name}"
        params = exp.runtime_params or {}
        
        # åŠ¨æ€åŠ è½½æ¨¡å‹
        student_model_name = params.get("student_model") or params.get("llm") or "qwen-flash"
        student_llm = setup_llm(student_model_name)
        
        judge_model_name = params.get("judge_model", "qwen-max")
        judge_llm = setup_llm(judge_model_name)
        
        embed_model = setup_embed_model(kb.embed_model)
        
        # VectorStore åˆå§‹åŒ– (éƒ¨åˆ†æ¶‰åŠç½‘ç»œï¼Œå¯è€ƒè™‘ to_threadï¼Œä½† ensure_collection ä¸»è¦æ˜¯æ£€æŸ¥)
        vector_store_manager = VectorStoreManager(f"kb_{kb.id}", embed_model)
        await asyncio.to_thread(vector_store_manager.ensure_collection)
        
        qa_service = QAService(student_llm) 
        
        pipeline = RAGPipeline.build(
            store_manager=vector_store_manager,
            qa_service=qa_service,
            top_k=params.get("top_k", settings.TOP_K),
            strategy=params.get("strategy", "default")
        )
        
        evaluator = RAGEvaluator(
            rag_pipeline=pipeline,
            llm=judge_llm, 
            embed_model=embed_model
        )

        try:
            await evaluator.adapt_metrics(language="chinese")
        except Exception as e:
            logger.error(f"æŒ‡æ ‡é€‚é…æµç¨‹å¼‚å¸¸: {e}")

        # 3. åŠ è½½æ•°æ®é›† (Langfuse API -> Thread)
        def _get_dataset():
            langfuse = Langfuse()
            return langfuse.get_dataset(dataset_name)
        
        lf_dataset = await asyncio.to_thread(_get_dataset)

        agg_scores = {"faithfulness": [], 
                      "answer_relevancy": [], 
                      "context_recall": [], 
                      "context_precision": [], 
                      "answer_accuracy": [], 
                      "context_entities_recall": []
                      }

        # 4. éå†å¹¶è¿è¡Œå®éªŒ
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å·²ç»æ˜¯ async å‡½æ•°äº†ï¼Œå¯ä»¥ç›´æ¥ await pipeline.async_query
        for item in lf_dataset.items:
            question = item.input
            ground_truth = item.expected_output
            
            # Langfuse çš„ trace ä¸Šä¸‹æ–‡ç®¡ç†
            with item.run(
                run_name=f"exp_{experiment_id}_{kb.name}",
                run_description=f"Strat: {params.get('strategy')} | Model: {student_model_name}",
                run_metadata={
                    "experiment_id": experiment_id,
                    "knowledge_id": kb.id,
                    **params
                }
            ) as trace:
                
                # A. æ‰§è¡Œ RAG Pipeline (å¼‚æ­¥)
                # pipeline.async_query å·²ç»æ˜¯åŸç”Ÿå¼‚æ­¥çš„
                answer_result, docs = await pipeline.async_query(question)
                retrieved_contexts = [d.page_content for d in docs]
                
                # B. è®¡ç®— Ragas åˆ†æ•° (å¼‚æ­¥)
                # evaluator.score_single_item å·²ç»æ˜¯åŸç”Ÿå¼‚æ­¥çš„
                scores = await evaluator.score_single_item(
                    question=question,
                    answer=answer_result,
                    contexts=retrieved_contexts,
                    ground_truth=ground_truth
                )
                
                safe_scores = {k: float(v) for k, v in scores.items()}
                
                # C. ä¸ŠæŠ¥åˆ†æ•°
                for metric_name, val in safe_scores.items():
                    trace.score(name=metric_name, value=val)
                    
                    # ğŸŸ¢ [å…³é”®ä¿®å¤] åç§°æ˜ å°„ï¼šå°† Ragas çš„åç§°æ˜ å°„åˆ° DB/agg_scores çš„åç§°
                    target_key = metric_name
                    
                    if metric_name == "context_entity_recall":
                        target_key = "context_entities_recall"
                
                    if metric_name == "nv_accuracy": 
                        target_key = "answer_accuracy"

                    # 3. åªæœ‰åœ¨ agg_scores å®šä¹‰äº†çš„æŒ‡æ ‡æ‰ç»Ÿè®¡
                    if target_key in agg_scores:
                        agg_scores[target_key].append(val)
                    else:
                        # æ–¹ä¾¿è°ƒè¯•ï¼Œæ‰“å°ä¸€ä¸‹ä¸åœ¨åˆ—è¡¨é‡Œçš„æŒ‡æ ‡å
                        logger.debug(f"Metric {metric_name} (mapped to {target_key}) not in agg_scores, skipping.")

        # 5. è®¡ç®—å¹³å‡åˆ†
        def avg(lst):
            return float(sum(lst) / len(lst)) if lst else 0.0

        # æ›´æ–° DB
        # é‡æ–°è·å–å¯¹è±¡
        exp = await db.get(Experiment, experiment_id)
        if exp:
            exp.faithfulness = avg(agg_scores["faithfulness"])
            exp.answer_relevancy = avg(agg_scores["answer_relevancy"])
            exp.context_recall = avg(agg_scores["context_recall"])
            exp.context_precision = avg(agg_scores["context_precision"])
            exp.answer_accuracy = avg(agg_scores["answer_accuracy"])
            exp.context_entities_recall = avg(agg_scores["context_entities_recall"])
            
            exp.status = "COMPLETED"
            db.add(exp)
            await db.commit()
            
        logger.info(f"å®éªŒ {experiment_id} å®Œæˆã€‚")

    except Exception as e:
        logger.error(f"å®éªŒ {experiment_id} å¤±è´¥: {e}", exc_info=True)
        await db.rollback()
        exp = await db.get(Experiment, experiment_id)
        if exp:
            exp.status = "FAILED"
            exp.error_message = str(e)[:500]
            db.add(exp)
            await db.commit()