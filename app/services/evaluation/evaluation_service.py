# app/services/evaluation_service.py
import asyncio
import logging
import nest_asyncio
import tempfile
from pathlib import Path
from typing import List
from sqlmodel import Session
from langfuse import Langfuse

# å¤ç”¨å·²æœ‰çš„ Ragas é€»è¾‘
from ragas.testset import TestsetGenerator
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.llms import LangchainLLMWrapper

# å¤ç”¨é¡¹ç›®åŸºç¡€è®¾æ–½
from app.core.config import settings
from app.domain.models import Testset, Experiment
from app.services.factories import setup_embed_model, setup_qwen_llm
from app.services.file_storage import save_bytes_to_minio, get_minio_client
from app.services.loader import load_single_document
from app.services.retrieval import VectorStoreManager
from app.services.pipelines import RAGPipeline
from app.services.generation import QAService
from app.services.evaluation.runner import RAGEvaluator

logger = logging.getLogger(__name__)

# åº”ç”¨ nest_asyncio é˜²æ­¢äº‹ä»¶å¾ªç¯å†²çª (Ragas å†…éƒ¨å¯èƒ½éœ€è¦)
nest_asyncio.apply()

# ==========================================
# 1. æµ‹è¯•é›†ç”Ÿæˆ (Generate Testset)
# ==========================================

def generate_testset_pipeline(db: Session, testset_id: int, source_doc_ids: List[int]):
    """
    æ ¹æ®æŒ‡å®šçš„æºæ–‡æ¡£ç”Ÿæˆæµ‹è¯•é›†ï¼Œå¹¶å­˜å…¥ MinIO å’Œ DB
    """
    from app.domain.models import Document as DBDocument # é¿å…å‘½åå†²çª
    
    langfuse = Langfuse()
    testset = db.get(Testset, testset_id)
    if not testset:
        logger.error(f"Testset {testset_id} not found")
        return

    try:
        logger.info(f"å¼€å§‹ä¸º Testset {testset_id} ç”Ÿæˆæ•°æ®ï¼Œæºæ–‡æ¡£ID: {source_doc_ids}")
        testset.status = "GENERATING"
        db.add(testset)
        db.commit()
        # 1. åŠ è½½æºæ–‡æ¡£ (ä» MinIO ä¸‹è½½ -> LangChain Document)
        langchain_docs = []
        minio_client = get_minio_client()
        
        for doc_id in source_doc_ids:
            db_doc = db.get(DBDocument, doc_id)
            if not db_doc: 
                continue
            
            # ä¸´æ—¶ä¸‹è½½æ–‡ä»¶ä»¥è¯»å–å†…å®¹
            suffix = Path(db_doc.filename).suffix
            with tempfile.NamedTemporaryFile(delete=True, suffix=suffix) as tmp:
                minio_client.fget_object(settings.MINIO_BUCKET_NAME, db_doc.file_path, tmp.name)
                loaded = load_single_document(tmp.name)
                langchain_docs.extend(loaded)
        
        if not langchain_docs:
            raise ValueError("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æ¡£ï¼Œæ— æ³•ç”Ÿæˆæµ‹è¯•é›†")

        # 2. åˆå§‹åŒ– Generator (å¤ç”¨ testset.py çš„é€»è¾‘)
        # æ³¨æ„ï¼šç”Ÿæˆæµ‹è¯•é›†é€šå¸¸éœ€è¦è¾ƒå¼ºçš„æ¨¡å‹ (Generator LLM)
        # è¿™é‡Œæš‚æ—¶å¤ç”¨ qwen-flashï¼Œå®é™…ç”Ÿäº§å»ºè®®æ¢æˆ qwen-max æˆ– gpt-4
        generator_llm = setup_qwen_llm("qwen-max") # å»ºè®®ç”¨å¼ºæ¨¡å‹
        generator_embed = setup_embed_model("text-embedding-v4")
        
        generator = TestsetGenerator(
            llm=LangchainLLMWrapper(generator_llm), 
            embedding_model=LangchainEmbeddingsWrapper(generator_embed)
        )
        
        # 3. æ‰§è¡Œç”Ÿæˆ (Ragas Core)
        # testset_size å¯ä»¥åœ¨ testset è¡¨é‡ŒåŠ å­—æ®µæ§åˆ¶ï¼Œè¿™é‡Œå…ˆå†™æ­»æˆ–è¯»é…ç½®
        dataset = generator.generate_with_langchain_docs(
            langchain_docs, 
            testset_size=settings.TESTSET_SIZE
        )
        
        # 4. è½¬ CSV å¹¶ä¿å­˜åˆ° MinIO
        df = dataset.to_pandas() #type: ignore
        json_str = df.to_json(orient="records", lines=True, force_ascii=False)
        json_bytes = json_str.encode('utf-8')
        
        # æ”¹åç¼€ä¸º .jsonl
        file_path = f"testsets/{testset.id}_{testset.name}.jsonl"
        # content_type æ”¹ä¸º json
        save_bytes_to_minio(json_bytes, file_path, "application/json")
        
        # ğŸŸ¢ 5. åŒæ­¥ä¸Šä¼ åˆ° Langfuse Datasets
        lf_dataset_name = f"testset_{testset.id}_{testset.name}"
        logger.info(f"æ­£åœ¨åŒæ­¥æµ‹è¯•é›†åˆ° Langfuse: {lf_dataset_name}")
        
        langfuse.create_dataset(
            name=lf_dataset_name,
            description=f"Auto-generated from docs: {source_doc_ids}",
            metadata={"testset_id": testset_id, "source": "rag-practice"}
        )
        
        # éå† DataFrame ä¸Šä¼  Item
        for _, row in df.iterrows():
            langfuse.create_dataset_item(
                dataset_name=lf_dataset_name,
                input=row["user_input"],          # Question
                expected_output=row["reference"], # Ground Truth
                metadata={
                    "source_context": row.get("reference_contexts")
                }
            )

        # 5. æ›´æ–° DB
        testset.file_path = file_path
        testset.description = f"Generated from {len(source_doc_ids)} docs. Size: {len(df)}"
        testset.status = "COMPLETED" # <--- æ ‡è®°å®Œæˆ
        testset.error_message = None
        db.add(testset)
        db.commit()
        logger.info(f"Testset {testset_id} ç”Ÿæˆå®Œæˆå¹¶ä¿å­˜åˆ° {file_path}")

    except Exception as e:
        logger.error(f"Testset ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
        # [ä¿®æ”¹] æ ‡è®°å¤±è´¥
        # é‡æ–°è·å–å¯¹è±¡é˜²æ­¢ session è„±ç¦»
        testset = db.get(Testset, testset_id) 
        if testset:
            testset.status = "FAILED"
            testset.error_message = str(e)
            db.add(testset)
            db.commit()
        raise e

# ==========================================
# 2. è¿è¡Œå®éªŒ (Run Experiment)
# ==========================================

def run_experiment_pipeline(db: Session, experiment_id: int):
    """
    æ‰§è¡Œ RAG è¯„æµ‹å®éªŒï¼šLangfuse Experiment Runner æ¨¡å¼
    """
    langfuse = Langfuse()
    
    exp = db.get(Experiment, experiment_id)
    if not exp:
        return

    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œå®éªŒ {experiment_id}...")
        exp.status = "RUNNING"
        db.add(exp)
        db.commit()

        # 1. å‡†å¤‡ç»„ä»¶
        kb = exp.knowledge
        ts = exp.testset
        
        dataset_name = f"testset_{ts.id}_{ts.name}"
        
        embed_model = setup_embed_model(kb.embed_model)
        vector_store_manager = VectorStoreManager(f"kb_{kb.id}", embed_model)
        vector_store_manager.ensure_collection()
        
        params = exp.runtime_params or {}
        student_llm = setup_qwen_llm(params.get("llm", "qwen-flash"))
        qa_service = QAService(student_llm) 
        
        pipeline = RAGPipeline.build(
            store_manager=vector_store_manager,
            qa_service=qa_service,
            top_k=params.get("top_k", settings.TOP_K),
            strategy=params.get("strategy", "default")
        )
        
        judge_llm = setup_qwen_llm("qwen-max", max_tokens=2048) 
        evaluator = RAGEvaluator(
            rag_pipeline=pipeline,
            llm=judge_llm,
            embed_model=embed_model
        )

        try:
            # ç¡®ä¿åœ¨ worker çº¿ç¨‹ä¸­è°ƒç”¨ï¼Œä¸ä¼šé˜»å¡ä¸»äº‹ä»¶å¾ªç¯
            asyncio.run(evaluator.adapt_metrics(language="chinese"))
        except Exception as e:
            logger.error(f"æŒ‡æ ‡é€‚é…æµç¨‹å¼‚å¸¸: {e}ï¼Œå®éªŒå°†ä½¿ç”¨é»˜è®¤ Prompt ç»§ç»­è¿è¡Œ")

        # 2. ä» Langfuse åŠ è½½æ•°æ®é›†
        logger.info(f"ä» Langfuse åŠ è½½æ•°æ®é›†: {dataset_name}")
        try:
            lf_dataset = langfuse.get_dataset(dataset_name)
        except Exception as e:
            raise ValueError(f"æ— æ³•åœ¨ Langfuse æ‰¾åˆ°æ•°æ®é›†: {dataset_name}ã€‚è¯·ç¡®è®¤è¯¥æµ‹è¯•é›†æ˜¯å¦å·²æˆåŠŸç”Ÿæˆå¹¶åŒæ­¥ã€‚")

        agg_scores = {"faithfulness": [], "answer_relevancy": [], "context_recall": [], "context_precision": []}

        # 3. éå†å¹¶è¿è¡Œå®éªŒ
        for item in lf_dataset.items:
            question = item.input
            ground_truth = item.expected_output
            
            with item.run(
                run_name=f"exp_{experiment_id}_{kb.name}",
                run_description=f"Strategy: {params.get('strategy')}",
                run_metadata={
                    "experiment_id": experiment_id,
                    "knowledge_id": kb.id,
                    **params
                }
            ) as trace:
                
                # A. æ‰§è¡Œ RAG Pipeline
                answer_result, docs = asyncio.run(pipeline.async_query(question))
                retrieved_contexts = [d.page_content for d in docs]
                
                # B. è®¡ç®— Ragas åˆ†æ•°
                scores = asyncio.run(evaluator.score_single_item(
                    question=question,
                    answer=answer_result,
                    contexts=retrieved_contexts,
                    ground_truth=ground_truth
                ))
                
                # ğŸŸ¢ [å…³é”®ä¿®å¤] å¼ºåˆ¶è½¬æ¢ä¸ºåŸç”Ÿ floatï¼Œé˜²æ­¢ numpy ç±»å‹æ±¡æŸ“
                safe_scores = {k: float(v) for k, v in scores.items()}
                
                # C. ä¸ŠæŠ¥åˆ†æ•°åˆ° Langfuse
                for metric_name, val in safe_scores.items():
                    trace.score(name=metric_name, value=val)
                    if metric_name in agg_scores:
                        agg_scores[metric_name].append(val)

        # 4. è®¡ç®—å¹³å‡åˆ†å¹¶æ›´æ–° DB
        def avg(lst):
            # å†æ¬¡ç¡®ä¿ç»“æœæ˜¯åŸç”Ÿ float
            return float(sum(lst) / len(lst)) if lst else 0.0

        exp.faithfulness = avg(agg_scores["faithfulness"])
        exp.answer_relevancy = avg(agg_scores["answer_relevancy"])
        exp.context_recall = avg(agg_scores["context_recall"])
        exp.context_precision = avg(agg_scores["context_precision"])
        
        exp.status = "COMPLETED"
        db.add(exp)
        db.commit()
        logger.info(f"å®éªŒ {experiment_id} å®Œæˆã€‚Avg Scores: Faith={exp.faithfulness:.2f}")

    except Exception as e:
        logger.error(f"å®éªŒ {experiment_id} å¤±è´¥: {e}", exc_info=True)
        # äº‹åŠ¡å›æ»šé˜²æ­¢æ±¡æŸ“
        db.rollback()
        # é‡æ–°è·å– exp å¯¹è±¡ä»¥è®°å½•é”™è¯¯
        exp = db.get(Experiment, experiment_id)
        if exp:
            exp.status = "FAILED"
            exp.error_message = str(e)[:500]
            db.add(exp)
            db.commit()