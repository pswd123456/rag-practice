import asyncio
import logging
import nest_asyncio
import tempfile
import pandas as pd
from pathlib import Path
from typing import List

from sqlmodel import select
from sqlmodel.ext.asyncio.session import AsyncSession
from sqlalchemy.orm import selectinload
from langfuse import Langfuse

# å¤ç”¨å·²æœ‰çš„ Ragas é€»è¾‘
from ragas.testset import TestsetGenerator
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.llms import LangchainLLMWrapper

from app.core.config import settings
from app.domain.models import Testset, Experiment, Document, Knowledge
from app.services.factories import setup_embed_model, setup_llm
from app.services.file_storage import save_bytes_to_minio, get_minio_client
from app.services.loader import load_single_document
from app.services.retrieval import VectorStoreManager
from app.services.pipelines import RAGPipeline
from app.services.generation import QAService
from app.services.evaluation.runner import RAGEvaluator

logger = logging.getLogger(__name__)

# åº”ç”¨ nest_asyncio é˜²æ­¢äº‹ä»¶å¾ªç¯å†²çª (Ragas å†…éƒ¨å¯èƒ½ç”¨åˆ° asyncio.run)
nest_asyncio.apply()

# ==========================================
# 1. æµ‹è¯•é›†ç”Ÿæˆ (Generate Testset)
# ==========================================

async def generate_testset_pipeline(db: AsyncSession, testset_id: int, source_doc_ids: List[int], generator_model: str = "qwen-max"):
    """
    æ ¹æ®æŒ‡å®šçš„æºæ–‡æ¡£ç”Ÿæˆæµ‹è¯•é›† (å¼‚æ­¥ç‰ˆ)
    """
    # 1. è·å– Testset
    testset = await db.get(Testset, testset_id)
    if not testset:
        logger.error(f"Testset {testset_id} not found")
        return

    try:
        logger.info(f"å¼€å§‹ä¸º Testset {testset_id} ç”Ÿæˆæ•°æ®ï¼Œæºæ–‡æ¡£ID: {source_doc_ids}")
        testset.status = "GENERATING"
        db.add(testset)
        await db.commit()
        
        # 2. åŠ è½½æºæ–‡æ¡£ (æ¶‰åŠ MinIO å’Œ æ–‡ä»¶åŠ è½½ï¼Œæ”¾å…¥ Thread)
        # ä¼˜åŒ–ï¼šå…ˆåœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­æŸ¥å‡ºæ‰€æœ‰ Document çš„ path
        doc_paths = []
        for doc_id in source_doc_ids:
            db_doc = await db.get(Document, doc_id)
            if db_doc:
                doc_paths.append((db_doc.filename, db_doc.file_path))
        
        if not doc_paths:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£è®°å½•")

        # å®šä¹‰é˜»å¡çš„åŠ è½½å‡½æ•°
        def _blocking_load():
            loaded_docs = []
            minio_client = get_minio_client()
            for filename, file_path in doc_paths:
                suffix = Path(filename).suffix
                with tempfile.NamedTemporaryFile(delete=True, suffix=suffix) as tmp:
                    
                    minio_client.fget_object(
                        bucket_name=settings.MINIO_BUCKET_NAME, 
                        object_name=file_path, 
                        file_path=tmp.name
                    )
                    # å¤ç”¨ loader
                    loaded = load_single_document(tmp.name)
                    loaded_docs.extend(loaded)
            return loaded_docs

        langchain_docs = await asyncio.to_thread(_blocking_load)
        
        if not langchain_docs:
            raise ValueError("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æ¡£å†…å®¹")

        # 3. æ‰§è¡Œç”Ÿæˆ (Ragas Generator æ˜¯é‡ CPU/IO æ“ä½œï¼Œä¸”å†…éƒ¨å¯èƒ½æœ‰ EventLoopï¼Œä½¿ç”¨ to_thread)
        def _generation_task():
            # åˆå§‹åŒ– Generator
            generator_llm = setup_llm(generator_model) 
            generator_embed = setup_embed_model("text-embedding-v4")
            
            generator = TestsetGenerator(
                llm=LangchainLLMWrapper(generator_llm), 
                embedding_model=LangchainEmbeddingsWrapper(generator_embed)
            )
            
            return generator.generate_with_langchain_docs(
                langchain_docs, 
                testset_size=settings.TESTSET_SIZE
            )

        logger.info("æ­£åœ¨è°ƒç”¨ Ragas ç”Ÿæˆæµ‹è¯•é›† (è¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´)...")
        dataset = await asyncio.to_thread(_generation_task)
        
        # 4. è½¬ CSV å¹¶ä¿å­˜åˆ° MinIO (Thread)
        def _save_task():
            df = dataset.to_pandas() # type: ignore
            json_str = df.to_json(orient="records", lines=True, force_ascii=False)
            json_bytes = json_str.encode('utf-8')
            
            file_path = f"testsets/{testset_id}_{testset.name}.jsonl"
            # save_bytes_to_minio å†…éƒ¨å·²ç»å°è£…äº† kwargs è°ƒç”¨
            save_bytes_to_minio(json_bytes, file_path, "application/json")
            
            # åŒæ­¥åˆ° Langfuse (å¯é€‰ï¼Œç½‘ç»œIO)
            langfuse = Langfuse()
            lf_dataset_name = f"testset_{testset_id}_{testset.name}"
            langfuse.create_dataset(
                name=lf_dataset_name,
                description=f"Auto-generated from docs: {source_doc_ids}. Model: {generator_model}",
                metadata={"testset_id": testset_id, "source": "rag-practice"}
            )
            for _, row in df.iterrows():
                langfuse.create_dataset_item(
                    dataset_name=lf_dataset_name,
                    input=row["user_input"],
                    expected_output=row["reference"],
                    metadata={"source_context": row.get("reference_contexts")}
                )
            return file_path, len(df)

        saved_path, count = await asyncio.to_thread(_save_task)

        # 5. æ›´æ–° DB
        # é‡æ–°è·å–å¯¹è±¡ä»¥é˜²è¿‡æœŸ
        testset = await db.get(Testset, testset_id)
        if testset:
            testset.file_path = saved_path
            testset.description = f"Generated by {generator_model}. Size: {count}"
            testset.status = "COMPLETED"
            testset.error_message = None
            db.add(testset)
            await db.commit()
        
        logger.info(f"Testset {testset_id} ç”Ÿæˆå®Œæˆ")

    except Exception as e:
        logger.error(f"Testset ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
        await db.rollback()
        testset = await db.get(Testset, testset_id) 
        if testset:
            testset.status = "FAILED"
            testset.error_message = str(e)
            db.add(testset)
            await db.commit()
        raise e

# ==========================================
# 2. è¿è¡Œå®éªŒ (Run Experiment)
# ==========================================

async def run_experiment_pipeline(db: AsyncSession, experiment_id: int):
    """
    æ‰§è¡Œ RAG è¯„æµ‹å®éªŒ (å¼‚æ­¥ç‰ˆ)
    """
    # 1. é¢„åŠ è½½ Knowledge å’Œ Testset
    stmt = select(Experiment).where(Experiment.id == experiment_id).options(
        selectinload(Experiment.knowledge),
        selectinload(Experiment.testset)
    )
    result = await db.exec(stmt)
    exp = result.first()
    
    if not exp:
        return

    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œå®éªŒ {experiment_id}...")
        exp.status = "RUNNING"
        db.add(exp)
        await db.commit()

        # 2. å‡†å¤‡ç»„ä»¶
        kb = exp.knowledge
        ts = exp.testset
        dataset_name = f"testset_{ts.id}_{ts.name}"
        params = exp.runtime_params or {}
        
        # åŠ¨æ€åŠ è½½æ¨¡å‹
        student_model_name = params.get("student_model") or params.get("llm") or "qwen-flash"
        student_llm = setup_llm(student_model_name)
        
        judge_model_name = params.get("judge_model", "qwen-max")
        judge_llm = setup_llm(judge_model_name)
        
        embed_model = setup_embed_model(kb.embed_model)
        
        # VectorStore åˆå§‹åŒ– (éƒ¨åˆ†æ¶‰åŠç½‘ç»œï¼Œå¯è€ƒè™‘ to_threadï¼Œä½† ensure_collection ä¸»è¦æ˜¯æ£€æŸ¥)
        vector_store_manager = VectorStoreManager(f"kb_{kb.id}", embed_model)
        await asyncio.to_thread(vector_store_manager.ensure_collection)
        
        qa_service = QAService(student_llm) 
        
        pipeline = RAGPipeline.build(
            store_manager=vector_store_manager,
            qa_service=qa_service,
            top_k=params.get("top_k", settings.TOP_K),
            strategy=params.get("strategy", "default")
        )
        
        evaluator = RAGEvaluator(
            rag_pipeline=pipeline,
            llm=judge_llm, 
            embed_model=embed_model
        )

        try:
            await evaluator.adapt_metrics(language="chinese")
        except Exception as e:
            logger.error(f"æŒ‡æ ‡é€‚é…æµç¨‹å¼‚å¸¸: {e}")

        # 3. åŠ è½½æ•°æ®é›† (Langfuse API -> Thread)
        def _get_dataset():
            langfuse = Langfuse()
            return langfuse.get_dataset(dataset_name)
        
        lf_dataset = await asyncio.to_thread(_get_dataset)

        agg_scores = {"faithfulness": [], 
                      "answer_relevancy": [], 
                      "context_recall": [], 
                      "context_precision": [], 
                      "answer_accuracy": [], 
                      "context_entities_recall": []
                      }

        # 4. éå†å¹¶è¿è¡Œå®éªŒ
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å·²ç»æ˜¯ async å‡½æ•°äº†ï¼Œå¯ä»¥ç›´æ¥ await pipeline.async_query
        for item in lf_dataset.items:
            question = item.input
            ground_truth = item.expected_output
            
            # Langfuse çš„ trace ä¸Šä¸‹æ–‡ç®¡ç†
            with item.run(
                run_name=f"exp_{experiment_id}_{kb.name}",
                run_description=f"Strat: {params.get('strategy')} | Model: {student_model_name}",
                run_metadata={
                    "experiment_id": experiment_id,
                    "knowledge_id": kb.id,
                    **params
                }
            ) as trace:
                
                # A. æ‰§è¡Œ RAG Pipeline (å¼‚æ­¥)
                # pipeline.async_query å·²ç»æ˜¯åŸç”Ÿå¼‚æ­¥çš„
                answer_result, docs = await pipeline.async_query(question)
                retrieved_contexts = [d.page_content for d in docs]
                
                # B. è®¡ç®— Ragas åˆ†æ•° (å¼‚æ­¥)
                # evaluator.score_single_item å·²ç»æ˜¯åŸç”Ÿå¼‚æ­¥çš„
                scores = await evaluator.score_single_item(
                    question=question,
                    answer=answer_result,
                    contexts=retrieved_contexts,
                    ground_truth=ground_truth
                )
                
                safe_scores = {k: float(v) for k, v in scores.items()}
                
                # C. ä¸ŠæŠ¥åˆ†æ•°
                for metric_name, val in safe_scores.items():
                    trace.score(name=metric_name, value=val)
                    
                    # ğŸŸ¢ [å…³é”®ä¿®å¤] åç§°æ˜ å°„ï¼šå°† Ragas çš„åç§°æ˜ å°„åˆ° DB/agg_scores çš„åç§°
                    target_key = metric_name
                    
                    if metric_name == "context_entity_recall":
                        target_key = "context_entities_recall"
                
                    if metric_name == "nv_accuracy": 
                        target_key = "answer_accuracy"

                    # 3. åªæœ‰åœ¨ agg_scores å®šä¹‰äº†çš„æŒ‡æ ‡æ‰ç»Ÿè®¡
                    if target_key in agg_scores:
                        agg_scores[target_key].append(val)
                    else:
                        # æ–¹ä¾¿è°ƒè¯•ï¼Œæ‰“å°ä¸€ä¸‹ä¸åœ¨åˆ—è¡¨é‡Œçš„æŒ‡æ ‡å
                        logger.debug(f"Metric {metric_name} (mapped to {target_key}) not in agg_scores, skipping.")

        # 5. è®¡ç®—å¹³å‡åˆ†
        def avg(lst):
            return float(sum(lst) / len(lst)) if lst else 0.0

        # æ›´æ–° DB
        # é‡æ–°è·å–å¯¹è±¡
        exp = await db.get(Experiment, experiment_id)
        if exp:
            exp.faithfulness = avg(agg_scores["faithfulness"])
            exp.answer_relevancy = avg(agg_scores["answer_relevancy"])
            exp.context_recall = avg(agg_scores["context_recall"])
            exp.context_precision = avg(agg_scores["context_precision"])
            exp.answer_accuracy = avg(agg_scores["answer_accuracy"])
            exp.context_entities_recall = avg(agg_scores["context_entities_recall"])
            
            exp.status = "COMPLETED"
            db.add(exp)
            await db.commit()
            
        logger.info(f"å®éªŒ {experiment_id} å®Œæˆã€‚")

    except Exception as e:
        logger.error(f"å®éªŒ {experiment_id} å¤±è´¥: {e}", exc_info=True)
        await db.rollback()
        exp = await db.get(Experiment, experiment_id)
        if exp:
            exp.status = "FAILED"
            exp.error_message = str(e)[:500]
            db.add(exp)
            await db.commit()