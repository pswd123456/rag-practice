# app/services/generation/qa_service.py
from __future__ import annotations

import logging
from typing import Any, Dict, Optional

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langfuse import Langfuse # ğŸŸ¢ ä»…å¯¼å…¥ä¸»å…¥å£å³å¯

logger = logging.getLogger(__name__)

class QAService:
    """
    è´Ÿè´£ Prompt æ„å»ºä¸ LLM è¾“å‡ºè§£æã€‚
    é›†æˆ Langfuse Prompt Management å®ç°äº‘ç«¯ Prompt ç‰ˆæœ¬ç®¡ç†ã€‚
    """

    def __init__(self, llm: Any, prompt_name: str = "rag-default"):
        """
        åˆå§‹åŒ– QA æœåŠ¡ã€‚
        
        :param llm: LangChain LLM å¯¹è±¡
        :param prompt_name: Langfuse ä¸­çš„ Prompt åç§° (é»˜è®¤: "rag-default")
        """
        self.llm = llm
        self.langfuse = Langfuse()
        self.langfuse_prompt_obj = None # ğŸŸ¢ ä¿å­˜ Prompt å¯¹è±¡å¤‡ç”¨
        
        try:
            # 1. ä» Langfuse äº‘ç«¯è·å– Prompt
            logger.info(f"æ­£åœ¨ä» Langfuse åŠ è½½ Prompt: {prompt_name}...")
            self.langfuse_prompt_obj = self.langfuse.get_prompt(prompt_name)
            
            # 2. è½¬æ¢ä¸º LangChain æ ¼å¼
            self.template = self.langfuse_prompt_obj.get_langchain_prompt()
            logger.info(f"Prompt åŠ è½½æˆåŠŸ (Version: {self.langfuse_prompt_obj.version})")
            
        except Exception as e:
            logger.error(f"âŒ Langfuse Prompt åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°æœ¬åœ°é»˜è®¤ Prompt: {e}", exc_info=True)
            # Fallback (å…œåº•é€»è¾‘)
            self.template = """
            ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
            å¦‚æœæ— æ³•å›ç­”ï¼Œè¯·ç›´æ¥è¯´æ˜ã€‚
            
            ä¸Šä¸‹æ–‡:
            {context}
            
            é—®é¢˜:
            {question}
            """.strip()

        self.prompt = ChatPromptTemplate.from_template(self.template)
        self.output_parser = StrOutputParser()
        
        # æ„å»º Chain
        self.chain = self.prompt | self.llm | self.output_parser
        
        logger.debug("QAService é“¾æ„å»ºå®Œæˆã€‚")

    def format_inputs(self, question: str, context: str) -> Dict[str, str]:
        return {"question": question, "context": context}

    def invoke(self, question: str, context: str, config: Optional[RunnableConfig] = None) -> str:
        """åŒæ­¥è°ƒç”¨"""
        # å¤ç”¨å¼‚æ­¥çš„é…ç½®é€»è¾‘ï¼ˆå¦‚æœæœ‰éœ€è¦ï¼Œä¹Ÿå¯ä»¥å•ç‹¬å†™ï¼‰
        config = self._inject_prompt_metadata(config)
        payload = self.format_inputs(question, context)
        return self.chain.invoke(payload, config=config)

    async def ainvoke(self, question: str, context: str, config: Optional[RunnableConfig] = None) -> str:
        """
        å¼‚æ­¥è°ƒç”¨ (æ”¯æŒä¼ å…¥ config ä»¥å¯ç”¨ Tracing)
        """
        # ğŸŸ¢ [ä¿®æ”¹ç‚¹] æ³¨å…¥ Prompt Metadata
        # Langfuse CallbackHandler ä¼šè‡ªåŠ¨è¯»å–è¿™ä¸ª metadata å¹¶è¿›è¡Œå…³è”
        config = self._inject_prompt_metadata(config)

        payload = self.format_inputs(question, context)
        return await self.chain.ainvoke(payload, config=config)

    def _inject_prompt_metadata(self, config: Optional[RunnableConfig]) -> RunnableConfig:
        """
        è¾…åŠ©æ–¹æ³•ï¼šå°† Langfuse Prompt å¯¹è±¡æ³¨å…¥åˆ° metadata ä¸­
        """
        # ç¡®ä¿ config æ˜¯ä¸€ä¸ªå­—å…¸
        new_config = config.copy() if config else {}
        
        if self.langfuse_prompt_obj:
            # ç¡®ä¿ metadata å­˜åœ¨
            if "metadata" not in new_config:
                new_config["metadata"] = {}
            
            # ğŸŸ¢ å…³é”®ï¼šå°† Prompt å¯¹è±¡æ”¾å…¥ metadataï¼ŒKey å¿…é¡»æ˜¯ 'langfuse_prompt'
            new_config["metadata"]["langfuse_prompt"] = self.langfuse_prompt_obj
            
        return new_config