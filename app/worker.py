# app/worker.py

import os
import logging
from typing import Any, List
from arq.connections import RedisSettings
from sqlmodel import select, func, col

from app.core.config import settings
from app.db.session import async_session_maker, engine
from app.core.logging_setup import setup_logging

# Services
from app.services.ingest.ingest import process_document_pipeline
from app.services.knowledge.knowledge_crud import delete_knowledge_pipeline 
from app.services.evaluation.evaluation_service import generate_testset_pipeline, run_experiment_pipeline

# Models for State Checking
from app.domain.models import Document, DocStatus, Testset, Experiment, Knowledge, KnowledgeStatus

# --- 1. åˆå§‹åŒ– Worker æ—¥å¿— ---
setup_logging(str(settings.LOG_FILE_PATH), log_level="INFO")
logger = logging.getLogger("app.worker")

async def check_and_fix_zombie_tasks():
    """
    [Self-Healing] æ£€æŸ¥å¹¶ä¿®å¤å›  Worker å´©æºƒæˆ–é‡å¯è€Œæ®‹ç•™çš„ 'åƒµå°¸ä»»åŠ¡'ã€‚
    ç­–ç•¥å‡çº§ï¼šæ¸…ç†æ‰€æœ‰å¤„äº éç»ˆæ€ (COMPLETED/FAILED) ä¸” éç­‰å¾…æ€ (PENDING) çš„ä»»åŠ¡ã€‚
    è¿™æ„å‘³ç€ PROCESSING, GENERATING, DELETING ä»¥åŠä»»ä½•è‡ªå®šä¹‰çš„ä¸­é—´çŠ¶æ€ (å¦‚ DOCLING_PROCESSING) éƒ½ä¼šè¢«é‡ç½®ã€‚
    """
    logger.info("ğŸš‘ æ­£åœ¨æ£€æŸ¥åƒµå°¸ä»»åŠ¡ (Zombie Tasks)...")
    
    docs_to_fix = []
    testsets_to_fix = []
    exps_to_fix = []
    kbs_to_fix = []

    async with async_session_maker() as db:
        try:
            # --- è¯Šæ–­ï¼šæ‰“å°å½“å‰æ–‡æ¡£çŠ¶æ€åˆ†å¸ƒ ---
            # è¿™æœ‰åŠ©äºæ’æŸ¥ä¸ºä»€ä¹ˆæŸäº›æ–‡æ¡£æ²¡è¢«æ£€æµ‹åˆ°
            try:
                stats_stmt = select(Document.status, func.count(Document.id)).group_by(Document.status)
                stats = (await db.exec(stats_stmt)).all()
                if stats:
                    stats_dict = {str(s): c for s, c in stats}
                    logger.info(f"ğŸ“Š [DBè¯Šæ–­] å½“å‰æ–‡æ¡£çŠ¶æ€åˆ†å¸ƒ: {stats_dict}")
            except Exception as diag_err:
                logger.warning(f"æ— æ³•è·å–çŠ¶æ€ç»Ÿè®¡ä¿¡æ¯: {diag_err}")

            # --- 1. ä¿®å¤ Documents ---
            # é€»è¾‘ï¼šStatus NOT IN [COMPLETED, FAILED, PENDING] -> è§†ä¸ºåƒµå°¸ä»»åŠ¡
            # è¿™æ ·å¯ä»¥æ•è· PROCESSING ä»¥åŠç”¨æˆ·å¯èƒ½çš„è‡ªå®šä¹‰çŠ¶æ€ (å¦‚ DOCLING_PROCESSING)
            safe_statuses = [DocStatus.COMPLETED, DocStatus.FAILED, DocStatus.PENDING]
            # æ³¨æ„ï¼šæŸäº›æ•°æ®åº“å¯èƒ½éœ€è¦å°† Enum è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒï¼Œè¿™é‡Œä½¿ç”¨ col() è¾…åŠ©
            stmt_doc = select(Document).where(col(Document.status).notin_(safe_statuses))
            
            docs_to_fix = (await db.exec(stmt_doc)).all()
            if docs_to_fix:
                logger.warning(f"âš ï¸ å‘ç° {len(docs_to_fix)} ä¸ªå¤„äºä¸­é—´çŠ¶æ€çš„æ–‡æ¡£ (é COMPLETED/FAILED/PENDING)ï¼Œæ­£åœ¨é‡ç½®...")
                for doc in docs_to_fix:
                    original_status = doc.status
                    doc.status = DocStatus.FAILED
                    doc.error_message = f"ä»»åŠ¡å¼‚å¸¸ä¸­æ–­ (åŸçŠ¶æ€: {original_status}): æœåŠ¡å¯èƒ½å‘ç”Ÿäº†é‡å¯æˆ–å´©æºƒã€‚"
                    db.add(doc)
            
            # --- 2. ä¿®å¤ Testsets ---
            # Testset åªæœ‰ COMPLETED å’Œ FAILED æ˜¯ç»ˆæ€ (PENDING æ˜¯ç­‰å¾…æ€? å‡è®¾ GENERATING æ˜¯ä¸­é—´æ€)
            # åŸé€»è¾‘åªæŸ¥äº† GENERATINGï¼Œè¿™é‡Œä¿æŒå®½å®¹ï¼Œåªé‡ç½®æ˜ç¡®çš„ GENERATING
            stmt_ts = select(Testset).where(Testset.status == "GENERATING")
            testsets_to_fix = (await db.exec(stmt_ts)).all()
            if testsets_to_fix:
                logger.warning(f"âš ï¸ å‘ç° {len(testsets_to_fix)} ä¸ªå¡åœ¨ GENERATING çŠ¶æ€çš„æµ‹è¯•é›†ï¼Œæ­£åœ¨é‡ç½®...")
                for ts in testsets_to_fix:
                    ts.status = "FAILED"
                    ts.error_message = "ä»»åŠ¡å¼‚å¸¸ä¸­æ–­: æœåŠ¡å¯èƒ½å‘ç”Ÿäº†é‡å¯æˆ–å´©æºƒã€‚"
                    db.add(ts)

            # --- 3. ä¿®å¤ Experiments ---
            stmt_exp = select(Experiment).where(Experiment.status == "RUNNING")
            exps_to_fix = (await db.exec(stmt_exp)).all()
            if exps_to_fix:
                logger.warning(f"âš ï¸ å‘ç° {len(exps_to_fix)} ä¸ªå¡åœ¨ RUNNING çŠ¶æ€çš„å®éªŒï¼Œæ­£åœ¨é‡ç½®...")
                for exp in exps_to_fix:
                    exp.status = "FAILED"
                    exp.error_message = "ä»»åŠ¡å¼‚å¸¸ä¸­æ–­: æœåŠ¡å¯èƒ½å‘ç”Ÿäº†é‡å¯æˆ–å´©æºƒã€‚"
                    db.add(exp)
            
            # --- 4. ä¿®å¤ Knowledge Deletions ---
            stmt_kb = select(Knowledge).where(Knowledge.status == KnowledgeStatus.DELETING)
            kbs_to_fix = (await db.exec(stmt_kb)).all()
            if kbs_to_fix:
                logger.warning(f"âš ï¸ å‘ç° {len(kbs_to_fix)} ä¸ªå¡åœ¨ DELETING çŠ¶æ€çš„çŸ¥è¯†åº“ï¼Œæ­£åœ¨æ ‡è®°ä¸º FAILED...")
                for kb in kbs_to_fix:
                    kb.status = KnowledgeStatus.FAILED
                    db.add(kb)

            # æäº¤æ›´æ”¹
            if docs_to_fix or testsets_to_fix or exps_to_fix or kbs_to_fix:
                await db.commit()
                total_fixed = len(docs_to_fix) + len(testsets_to_fix) + len(exps_to_fix) + len(kbs_to_fix)
                logger.info(f"âœ… åƒµå°¸ä»»åŠ¡ä¿®å¤å®Œæˆï¼Œå…±ä¿®å¤ {total_fixed} é¡¹ã€‚")
            else:
                logger.info("âœ¨ æœªå‘ç°åƒµå°¸ä»»åŠ¡ï¼Œç³»ç»ŸçŠ¶æ€å¥åº·ã€‚")
                
        except Exception as e:
            logger.error(f"âŒ æ‰§è¡Œåƒµå°¸ä»»åŠ¡ä¿®å¤æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            await db.rollback()

async def startup(ctx: Any):
    logger.info("ğŸ‘· Worker è¿›ç¨‹å¯åŠ¨...")
    # æ‰§è¡Œè‡ªæ„ˆé€»è¾‘
    await check_and_fix_zombie_tasks()

async def shutdown(ctx: Any):
    logger.info("ğŸ‘· Worker è¿›ç¨‹å…³é—­...")
    await engine.dispose()

# --- Worker ä»»åŠ¡å®šä¹‰ (çº¯å¼‚æ­¥ï¼Œæ—  Wrapper) ---

async def process_document_task(ctx: Any, doc_id: int):
    logger.info(f"[Task] å¼€å§‹å¤„ç†æ–‡æ¡£: ID {doc_id}")
    # æ•°æ®åº“è¿æ¥ç°åœ¨ç”± pipeline å†…éƒ¨æŒ‰éœ€è·å–ï¼Œé˜²æ­¢ Docling ç­‰é•¿ä»»åŠ¡å ç”¨è¿æ¥æ± 
    try:
        await process_document_pipeline(doc_id)
    except Exception as e:
        logger.error(f"[Task] æ–‡æ¡£å¤„ç†å¼‚å¸¸ (ID {doc_id}): {e}", exc_info=True)

# å¢åŠ è¶…æ—¶æ—¶é—´
process_document_task.max_tries = 3 # type: ignore
process_document_task.retry_delay = 5 # type: ignore
process_document_task.timeout = 600 # type: ignore

async def delete_knowledge_task(ctx: Any, knowledge_id: int, user_id: int):
    logger.info(f"[Task] å¼€å§‹åˆ é™¤çŸ¥è¯†åº“: ID {knowledge_id} (User: {user_id})")
    async with async_session_maker() as db:
        try:
            # é€ä¼  user_id ç»™ pipeline
            await delete_knowledge_pipeline(db, knowledge_id, user_id)
        except Exception as e:
            logger.error(f"[Task] çŸ¥è¯†åº“åˆ é™¤å¼‚å¸¸ (ID {knowledge_id}): {e}", exc_info=True)

delete_knowledge_task.max_tries = 3 # type: ignore
delete_knowledge_task.retry_delay = 2 # type: ignore

async def generate_testset_task(ctx: Any, testset_id: int, source_doc_ids: List[int], generator_model: str = "qwen-max"):
    logger.info(f"[Task] å¼€å§‹ç”Ÿæˆæµ‹è¯•é›†: ID {testset_id}")
    async with async_session_maker() as db:
        try:
            await generate_testset_pipeline(db, testset_id, source_doc_ids, generator_model)
        except Exception as e:
            logger.error(f"[Task] æµ‹è¯•é›†ç”Ÿæˆå¼‚å¸¸ (ID {testset_id}): {e}", exc_info=True)

generate_testset_task.max_tries = 3 # type: ignore
generate_testset_task.retry_delay = 10 # type: ignore

async def run_experiment_task(ctx: Any, experiment_id: int):
    logger.info(f"[Task] å¼€å§‹è¿è¡Œå®éªŒ: ID {experiment_id}")
    async with async_session_maker() as db:
        try:
            await run_experiment_pipeline(db, experiment_id)
        except Exception as e:
            logger.error(f"[Task] å®éªŒè¿è¡Œå¼‚å¸¸ (ID {experiment_id}): {e}", exc_info=True)

run_experiment_task.max_tries = 3 # type: ignore
run_experiment_task.retry_delay = 10 # type: ignore

# --- Arq é…ç½® ---

class WorkerSettings:
    functions = [
        process_document_task, 
        delete_knowledge_task, 
        generate_testset_task, 
        run_experiment_task
    ]
    redis_settings = RedisSettings(
        host=settings.REDIS_HOST, 
        port=settings.REDIS_PORT
        )
    
    queue_name = os.getenv("ARQ_QUEUES", settings.DEFAULT_QUEUE_NAME)
    max_jobs = 1
    on_startup = startup
    on_shutdown = shutdown