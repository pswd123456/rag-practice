# app/worker.py

import os
import logging
from typing import Any, List
from datetime import datetime, timedelta, timezone # ğŸŸ¢ æ–°å¢
from arq import cron # ğŸŸ¢ æ–°å¢
from arq.connections import RedisSettings
from sqlmodel import select, func, col

from app.core.config import settings
from app.db.session import async_session_maker, engine
from app.core.logging_setup import setup_logging

# Services
from app.services.ingest.ingest import process_document_pipeline
from app.services.knowledge.knowledge_crud import delete_knowledge_pipeline 
from app.services.evaluation.evaluation_service import generate_testset_pipeline, run_experiment_pipeline

# Models for State Checking
from app.domain.models import Document, DocStatus, Testset, Experiment, Knowledge, KnowledgeStatus

# --- 1. åˆå§‹åŒ– Worker æ—¥å¿— ---
setup_logging(str(settings.LOG_FILE_PATH), log_level="INFO")
logger = logging.getLogger("app.worker")

# ... [ä¿ç•™åŸæœ‰çš„ check_and_fix_zombie_tasks å‡½æ•°ä¸å˜] ...
async def check_and_fix_zombie_tasks():
    # (æ­¤å¤„ä»£ç ä¿æŒåŸæ ·ï¼Œå®ƒæ˜¯å¯åŠ¨æ—¶çš„å…¨é‡æ¸…ç†)
    """
    [Self-Healing] æ£€æŸ¥å¹¶ä¿®å¤å›  Worker å´©æºƒæˆ–é‡å¯è€Œæ®‹ç•™çš„ 'åƒµå°¸ä»»åŠ¡'ã€‚
    ç­–ç•¥å‡çº§ï¼šæ¸…ç†æ‰€æœ‰å¤„äº éç»ˆæ€ (COMPLETED/FAILED) ä¸” éç­‰å¾…æ€ (PENDING) çš„ä»»åŠ¡ã€‚
    è¿™æ„å‘³ç€ PROCESSING, GENERATING, DELETING ä»¥åŠä»»ä½•è‡ªå®šä¹‰çš„ä¸­é—´çŠ¶æ€ (å¦‚ DOCLING_PROCESSING) éƒ½ä¼šè¢«é‡ç½®ã€‚
    """
    logger.info("ğŸš‘ æ­£åœ¨æ£€æŸ¥åƒµå°¸ä»»åŠ¡ (Zombie Tasks)...")
    
    docs_to_fix = []
    testsets_to_fix = []
    exps_to_fix = []
    kbs_to_fix = []

    async with async_session_maker() as db:
        try:
            # --- è¯Šæ–­ï¼šæ‰“å°å½“å‰æ–‡æ¡£çŠ¶æ€åˆ†å¸ƒ ---
            # è¿™æœ‰åŠ©äºæ’æŸ¥ä¸ºä»€ä¹ˆæŸäº›æ–‡æ¡£æ²¡è¢«æ£€æµ‹åˆ°
            try:
                stats_stmt = select(Document.status, func.count(Document.id)).group_by(Document.status)
                stats = (await db.exec(stats_stmt)).all()
                if stats:
                    stats_dict = {str(s): c for s, c in stats}
                    logger.info(f"ğŸ“Š [DBè¯Šæ–­] å½“å‰æ–‡æ¡£çŠ¶æ€åˆ†å¸ƒ: {stats_dict}")
            except Exception as diag_err:
                logger.warning(f"æ— æ³•è·å–çŠ¶æ€ç»Ÿè®¡ä¿¡æ¯: {diag_err}")

            # --- 1. ä¿®å¤ Documents ---
            # é€»è¾‘ï¼šStatus NOT IN [COMPLETED, FAILED, PENDING] -> è§†ä¸ºåƒµå°¸ä»»åŠ¡
            # è¿™æ ·å¯ä»¥æ•è· PROCESSING ä»¥åŠç”¨æˆ·å¯èƒ½çš„è‡ªå®šä¹‰çŠ¶æ€ (å¦‚ DOCLING_PROCESSING)
            safe_statuses = [DocStatus.COMPLETED, DocStatus.FAILED, DocStatus.PENDING]
            # æ³¨æ„ï¼šæŸäº›æ•°æ®åº“å¯èƒ½éœ€è¦å°† Enum è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒï¼Œè¿™é‡Œä½¿ç”¨ col() è¾…åŠ©
            stmt_doc = select(Document).where(col(Document.status).notin_(safe_statuses))
            
            docs_to_fix = (await db.exec(stmt_doc)).all()
            if docs_to_fix:
                logger.warning(f"âš ï¸ å‘ç° {len(docs_to_fix)} ä¸ªå¤„äºä¸­é—´çŠ¶æ€çš„æ–‡æ¡£ (é PENDING/COMPLETED/FAILED)ï¼Œæ­£åœ¨é‡ç½®...")
                for doc in docs_to_fix:
                    original_status = doc.status
                    doc.status = DocStatus.FAILED
                    doc.error_message = f"ä»»åŠ¡å¼‚å¸¸ä¸­æ–­ (åŸçŠ¶æ€: {original_status}): æœåŠ¡å¯èƒ½å‘ç”Ÿäº†é‡å¯æˆ–å´©æºƒã€‚"
                    db.add(doc)
            
            # --- 2. ä¿®å¤ Testsets ---
            # Testset åªæœ‰ COMPLETED å’Œ FAILED æ˜¯ç»ˆæ€ (PENDING æ˜¯ç­‰å¾…æ€? å‡è®¾ GENERATING æ˜¯ä¸­é—´æ€)
            # åŸé€»è¾‘åªæŸ¥äº† GENERATINGï¼Œè¿™é‡Œä¿æŒå®½å®¹ï¼Œåªé‡ç½®æ˜ç¡®çš„ GENERATING
            stmt_ts = select(Testset).where(Testset.status == "GENERATING")
            testsets_to_fix = (await db.exec(stmt_ts)).all()
            if testsets_to_fix:
                logger.warning(f"âš ï¸ å‘ç° {len(testsets_to_fix)} ä¸ªå¡åœ¨ GENERATING çŠ¶æ€çš„æµ‹è¯•é›†ï¼Œæ­£åœ¨é‡ç½®...")
                for ts in testsets_to_fix:
                    ts.status = "FAILED"
                    ts.error_message = "ä»»åŠ¡å¼‚å¸¸ä¸­æ–­: æœåŠ¡å¯èƒ½å‘ç”Ÿäº†é‡å¯æˆ–å´©æºƒã€‚"
                    db.add(ts)

            # --- 3. ä¿®å¤ Experiments ---
            stmt_exp = select(Experiment).where(Experiment.status == "RUNNING")
            exps_to_fix = (await db.exec(stmt_exp)).all()
            if exps_to_fix:
                logger.warning(f"âš ï¸ å‘ç° {len(exps_to_fix)} ä¸ªå¡åœ¨ RUNNING çŠ¶æ€çš„å®éªŒï¼Œæ­£åœ¨é‡ç½®...")
                for exp in exps_to_fix:
                    exp.status = "FAILED"
                    exp.error_message = "ä»»åŠ¡å¼‚å¸¸ä¸­æ–­: æœåŠ¡å¯èƒ½å‘ç”Ÿäº†é‡å¯æˆ–å´©æºƒã€‚"
                    db.add(exp)
            
            # --- 4. ä¿®å¤ Knowledge Deletions ---
            stmt_kb = select(Knowledge).where(Knowledge.status == KnowledgeStatus.DELETING)
            kbs_to_fix = (await db.exec(stmt_kb)).all()
            if kbs_to_fix:
                logger.warning(f"âš ï¸ å‘ç° {len(kbs_to_fix)} ä¸ªå¡åœ¨ DELETING çŠ¶æ€çš„çŸ¥è¯†åº“ï¼Œæ­£åœ¨æ ‡è®°ä¸º FAILED...")
                for kb in kbs_to_fix:
                    kb.status = KnowledgeStatus.FAILED
                    db.add(kb)

            # æäº¤æ›´æ”¹
            if docs_to_fix or testsets_to_fix or exps_to_fix or kbs_to_fix:
                await db.commit()
                total_fixed = len(docs_to_fix) + len(testsets_to_fix) + len(exps_to_fix) + len(kbs_to_fix)
                logger.info(f"âœ… åƒµå°¸ä»»åŠ¡ä¿®å¤å®Œæˆï¼Œå…±ä¿®å¤ {total_fixed} é¡¹ã€‚")
            else:
                logger.info("âœ¨ æœªå‘ç°åƒµå°¸ä»»åŠ¡ï¼Œç³»ç»ŸçŠ¶æ€å¥åº·ã€‚")
                
        except Exception as e:
            logger.error(f"âŒ æ‰§è¡Œåƒµå°¸ä»»åŠ¡ä¿®å¤æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            await db.rollback()

# -----------------------------------------------------------
# [New] ä¸»åŠ¨æ¸…ç†æœºåˆ¶ (Cron Job)
# -----------------------------------------------------------
async def fix_stale_tasks(ctx: Any):
    """
    [Watchdog] å®šæ—¶å·¡æ£€ä»»åŠ¡ã€‚
    æ¸…ç†æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼ˆè¶…è¿‡é˜ˆå€¼ï¼‰çš„ä»»åŠ¡ï¼Œé˜²æ­¢ä»»åŠ¡åœ¨è¿è¡Œæ—¶å¡æ­»ã€‚
    """
    # é˜ˆå€¼è®¾å®šï¼š1 å°æ—¶ã€‚å³ä½¿æ˜¯å¤§æ–‡ä»¶ Docling è§£æï¼Œä¹Ÿä¸åº”è¯¥è¶…è¿‡ 1 å°æ—¶ã€‚
    TIMEOUT_HOURS = 1
    # æ³¨æ„ï¼šä½¿ç”¨ utcnow è¿˜æ˜¯ now å–å†³äºæ•°æ®åº“æ—¶åŒºè®¾ç½®ï¼Œè¿™é‡Œå‡è®¾ naive datetime æˆ– local time
    # ä¸ºäº†ä¿é™©ï¼Œé€šå¸¸å»ºè®®æ•°æ®åº“ç»Ÿä¸€å­˜ UTCï¼Œè¿™é‡Œä½¿ç”¨ datetime.now() é€‚é…å¤§å¤šæ•°é»˜è®¤é…ç½®
    threshold_time = datetime.now() - timedelta(hours=TIMEOUT_HOURS)
    
    async with async_session_maker() as db:
        try:
            # 1. æ‰«æè¶…æ—¶æ–‡æ¡£ (çŠ¶æ€ä¸º PROCESSING ä¸” æ›´æ–°æ—¶é—´æ—©äº 1 å°æ—¶å‰)
            stmt_doc = select(Document).where(
                col(Document.status).notin_([DocStatus.COMPLETED, DocStatus.FAILED, DocStatus.PENDING]),
                Document.updated_at < threshold_time
            )
            stale_docs = (await db.exec(stmt_doc)).all()
            
            for doc in stale_docs:
                logger.warning(f"â° å‘ç°è¶…æ—¶ä»»åŠ¡: æ–‡æ¡£ {doc.id} (Status: {doc.status}) å·²å¡ä½è¶…è¿‡ {TIMEOUT_HOURS} å°æ—¶ï¼Œå¼ºåˆ¶ç½®ä¸ºå¤±è´¥ã€‚")
                doc.status = DocStatus.FAILED
                doc.error_message = f"ä»»åŠ¡è¶…æ—¶ (Watchdog): æ‰§è¡Œæ—¶é—´è¶…è¿‡ {TIMEOUT_HOURS} å°æ—¶ã€‚"
                db.add(doc)

            # 2. æ‰«æè¶…æ—¶æµ‹è¯•é›† (Testset æ²¡æœ‰ updated_atï¼Œä½¿ç”¨ created_at è¿‘ä¼¼)
            stmt_ts = select(Testset).where(
                Testset.status == "GENERATING",
                Testset.created_at < threshold_time
            )
            stale_ts = (await db.exec(stmt_ts)).all()
            for ts in stale_ts:
                logger.warning(f"â° å‘ç°è¶…æ—¶ä»»åŠ¡: æµ‹è¯•é›† {ts.id} ç”Ÿæˆè€—æ—¶è¿‡é•¿ï¼Œå¼ºåˆ¶ç½®ä¸ºå¤±è´¥ã€‚")
                ts.status = "FAILED"
                ts.error_message = "ä»»åŠ¡è¶…æ—¶ (Watchdog)"
                db.add(ts)

            # 3. æ‰«æè¶…æ—¶å®éªŒ
            stmt_exp = select(Experiment).where(
                Experiment.status == "RUNNING",
                Experiment.created_at < threshold_time
            )
            stale_exps = (await db.exec(stmt_exp)).all()
            for exp in stale_exps:
                logger.warning(f"â° å‘ç°è¶…æ—¶ä»»åŠ¡: å®éªŒ {exp.id} è¿è¡Œè€—æ—¶è¿‡é•¿ï¼Œå¼ºåˆ¶ç½®ä¸ºå¤±è´¥ã€‚")
                exp.status = "FAILED"
                exp.error_message = "ä»»åŠ¡è¶…æ—¶ (Watchdog)"
                db.add(exp)

            if stale_docs or stale_ts or stale_exps:
                await db.commit()
                logger.info("âœ… Watchdog æ¸…ç†å®Œæˆã€‚")
                
        except Exception as e:
            logger.error(f"Watchdog å·¡æ£€å¼‚å¸¸: {e}", exc_info=True)
            await db.rollback()

async def startup(ctx: Any):
    logger.info("ğŸ‘· Worker è¿›ç¨‹å¯åŠ¨...")
    # å¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡å…¨é‡æ¸…ç† (åŸºäºçŠ¶æ€)
    await check_and_fix_zombie_tasks()

async def shutdown(ctx: Any):
    logger.info("ğŸ‘· Worker è¿›ç¨‹å…³é—­...")
    await engine.dispose()

# ... [Worker ä»»åŠ¡å®šä¹‰ä¿æŒä¸å˜] ...
async def process_document_task(ctx: Any, doc_id: int):
    logger.info(f"[Task] å¼€å§‹å¤„ç†æ–‡æ¡£: ID {doc_id}")
    try:
        await process_document_pipeline(doc_id)
    except Exception as e:
        logger.error(f"[Task] æ–‡æ¡£å¤„ç†å¼‚å¸¸ (ID {doc_id}): {e}", exc_info=True)

# å¢åŠ è¶…æ—¶æ—¶é—´
process_document_task.max_tries = 3 # type: ignore
process_document_task.retry_delay = 5 # type: ignore
process_document_task.timeout = 600 # type: ignore

async def delete_knowledge_task(ctx: Any, knowledge_id: int, user_id: int):
    logger.info(f"[Task] å¼€å§‹åˆ é™¤çŸ¥è¯†åº“: ID {knowledge_id} (User: {user_id})")
    async with async_session_maker() as db:
        try:
            # é€ä¼  user_id ç»™ pipeline
            await delete_knowledge_pipeline(db, knowledge_id, user_id)
        except Exception as e:
            logger.error(f"[Task] çŸ¥è¯†åº“åˆ é™¤å¼‚å¸¸ (ID {knowledge_id}): {e}", exc_info=True)

delete_knowledge_task.max_tries = 3 # type: ignore
delete_knowledge_task.retry_delay = 2 # type: ignore

async def generate_testset_task(ctx: Any, testset_id: int, source_doc_ids: List[int], generator_model: str = "qwen-max"):
    logger.info(f"[Task] å¼€å§‹ç”Ÿæˆæµ‹è¯•é›†: ID {testset_id}")
    async with async_session_maker() as db:
        try:
            await generate_testset_pipeline(db, testset_id, source_doc_ids, generator_model)
        except Exception as e:
            logger.error(f"[Task] æµ‹è¯•é›†ç”Ÿæˆå¼‚å¸¸ (ID {testset_id}): {e}", exc_info=True)

generate_testset_task.max_tries = 3 # type: ignore
generate_testset_task.retry_delay = 10 # type: ignore

async def run_experiment_task(ctx: Any, experiment_id: int):
    logger.info(f"[Task] å¼€å§‹è¿è¡Œå®éªŒ: ID {experiment_id}")
    async with async_session_maker() as db:
        try:
            await run_experiment_pipeline(db, experiment_id)
        except Exception as e:
            logger.error(f"[Task] å®éªŒè¿è¡Œå¼‚å¸¸ (ID {experiment_id}): {e}", exc_info=True)

run_experiment_task.max_tries = 0 # type: ignore
run_experiment_task.retry_delay = 10 # type: ignore
run_experiment_task.timeout = 6000

# --- Arq é…ç½® ---

class WorkerSettings:
    functions = [
        process_document_task, 
        delete_knowledge_task, 
        generate_testset_task, 
        run_experiment_task
    ]
    redis_settings = RedisSettings(
        host=settings.REDIS_HOST, 
        port=settings.REDIS_PORT
    )
    
    # ğŸŸ¢ [New] æ³¨å†Œå®šæ—¶ä»»åŠ¡
    # æ¯ 10 åˆ†é’Ÿè¿è¡Œä¸€æ¬¡ fix_stale_tasks
    cron_jobs = [
        cron(fix_stale_tasks, minute={0, 10, 20, 30, 40, 50})
    ]
    
    queue_name = os.getenv("ARQ_QUEUES", settings.DEFAULT_QUEUE_NAME)
    max_jobs = 1
    on_startup = startup
    on_shutdown = shutdown