import os
import logging
from typing import Any, List
from arq.connections import RedisSettings
from sqlmodel import select

from app.core.config import settings
from app.db.session import async_session_maker, engine
from app.core.logging_setup import setup_logging

# Services
# [Modified] å¼•å…¥æ–°çš„ pipeline
from app.services.ingest.ingest import process_document_pipeline
from app.services.knowledge.knowledge_crud import delete_knowledge_pipeline 
from app.services.evaluation.evaluation_service import generate_testset_pipeline, run_experiment_pipeline

# Models for State Checking
from app.domain.models import Document, DocStatus, Testset, Experiment, Knowledge, KnowledgeStatus

# --- 1. åˆå§‹åŒ– Worker æ—¥å¿— ---
setup_logging(str(settings.LOG_FILE_PATH), log_level="INFO")
logger = logging.getLogger("app.worker")

async def check_and_fix_zombie_tasks():
    """
    [Self-Healing] æ£€æŸ¥å¹¶ä¿®å¤å›  Worker å´©æºƒæˆ–é‡å¯è€Œæ®‹ç•™çš„ 'åƒµå°¸ä»»åŠ¡'ã€‚
    å°†æ‰€æœ‰å¤„äºä¸­é—´çŠ¶æ€çš„ä»»åŠ¡æ ‡è®°ä¸º FAILEDã€‚
    """
    logger.info("ğŸš‘ æ­£åœ¨æ£€æŸ¥åƒµå°¸ä»»åŠ¡ (Zombie Tasks)...")
    
    async with async_session_maker() as db:
        try:
            # 1. ä¿®å¤ Documents (PROCESSING -> FAILED)
            stmt_doc = select(Document).where(Document.status == DocStatus.PROCESSING)
            docs = (await db.exec(stmt_doc)).all()
            if docs:
                logger.warning(f"å‘ç° {len(docs)} ä¸ªå¡åœ¨ PROCESSING çŠ¶æ€çš„æ–‡æ¡£ï¼Œæ­£åœ¨é‡ç½®...")
                for doc in docs:
                    doc.status = DocStatus.FAILED
                    doc.error_message = "ä»»åŠ¡å¼‚å¸¸ä¸­æ–­: æœåŠ¡å¯èƒ½å‘ç”Ÿäº†é‡å¯æˆ–å´©æºƒã€‚"
                    db.add(doc)
            
            # 2. ä¿®å¤ Testsets (GENERATING -> FAILED)
            stmt_ts = select(Testset).where(Testset.status == "GENERATING")
            testsets = (await db.exec(stmt_ts)).all()
            if testsets:
                logger.warning(f"å‘ç° {len(testsets)} ä¸ªå¡åœ¨ GENERATING çŠ¶æ€çš„æµ‹è¯•é›†ï¼Œæ­£åœ¨é‡ç½®...")
                for ts in testsets:
                    ts.status = "FAILED"
                    ts.error_message = "ä»»åŠ¡å¼‚å¸¸ä¸­æ–­: æœåŠ¡å¯èƒ½å‘ç”Ÿäº†é‡å¯æˆ–å´©æºƒã€‚"
                    db.add(ts)

            # 3. ä¿®å¤ Experiments (RUNNING -> FAILED)
            stmt_exp = select(Experiment).where(Experiment.status == "RUNNING")
            exps = (await db.exec(stmt_exp)).all()
            if exps:
                logger.warning(f"å‘ç° {len(exps)} ä¸ªå¡åœ¨ RUNNING çŠ¶æ€çš„å®éªŒï¼Œæ­£åœ¨é‡ç½®...")
                for exp in exps:
                    exp.status = "FAILED"
                    exp.error_message = "ä»»åŠ¡å¼‚å¸¸ä¸­æ–­: æœåŠ¡å¯èƒ½å‘ç”Ÿäº†é‡å¯æˆ–å´©æºƒã€‚"
                    db.add(exp)
            
            # 4. ä¿®å¤ Knowledge Deletions (DELETING -> FAILED)
            stmt_kb = select(Knowledge).where(Knowledge.status == KnowledgeStatus.DELETING)
            kbs = (await db.exec(stmt_kb)).all()
            if kbs:
                logger.warning(f"å‘ç° {len(kbs)} ä¸ªå¡åœ¨ DELETING çŠ¶æ€çš„çŸ¥è¯†åº“ï¼Œæ­£åœ¨æ ‡è®°ä¸º FAILED...")
                for kb in kbs:
                    kb.status = KnowledgeStatus.FAILED
                    # Knowledge æ¨¡å‹æ²¡æœ‰ error_message å­—æ®µï¼Œåªèƒ½é€šè¿‡çŠ¶æ€ä¼ è¾¾
                    db.add(kb)

            await db.commit()
            if docs or testsets or exps or kbs:
                logger.info("âœ… åƒµå°¸ä»»åŠ¡ä¿®å¤å®Œæˆã€‚")
            else:
                logger.info("âœ¨ æœªå‘ç°åƒµå°¸ä»»åŠ¡ï¼Œç³»ç»ŸçŠ¶æ€å¥åº·ã€‚")
                
        except Exception as e:
            logger.error(f"æ‰§è¡Œåƒµå°¸ä»»åŠ¡ä¿®å¤æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            await db.rollback()

async def startup(ctx: Any):
    logger.info("ğŸ‘· Worker è¿›ç¨‹å¯åŠ¨...")
    # æ‰§è¡Œè‡ªæ„ˆé€»è¾‘
    await check_and_fix_zombie_tasks()

async def shutdown(ctx: Any):
    logger.info("ğŸ‘· Worker è¿›ç¨‹å…³é—­...")
    await engine.dispose()

# --- Worker ä»»åŠ¡å®šä¹‰ (çº¯å¼‚æ­¥ï¼Œæ—  Wrapper) ---

async def process_document_task(ctx: Any, doc_id: int):
    logger.info(f"[Task] å¼€å§‹å¤„ç†æ–‡æ¡£: ID {doc_id}")
    # [Optimization] ç§»é™¤å¤–éƒ¨çš„ Session Context
    # æ•°æ®åº“è¿æ¥ç°åœ¨ç”± pipeline å†…éƒ¨æŒ‰éœ€è·å–ï¼Œé˜²æ­¢ Docling ç­‰é•¿ä»»åŠ¡å ç”¨è¿æ¥æ± 
    try:
        await process_document_pipeline(doc_id)
    except Exception as e:
        logger.error(f"[Task] æ–‡æ¡£å¤„ç†å¼‚å¸¸ (ID {doc_id}): {e}", exc_info=True)

# å¢åŠ è¶…æ—¶æ—¶é—´
process_document_task.max_tries = 3 # type: ignore
process_document_task.retry_delay = 5 # type: ignore
process_document_task.timeout = 600 # type: ignore

async def delete_knowledge_task(ctx: Any, knowledge_id: int):
    logger.info(f"[Task] å¼€å§‹åˆ é™¤çŸ¥è¯†åº“: ID {knowledge_id}")
    async with async_session_maker() as db:
        try:
            await delete_knowledge_pipeline(db, knowledge_id)
        except Exception as e:
            logger.error(f"[Task] çŸ¥è¯†åº“åˆ é™¤å¼‚å¸¸ (ID {knowledge_id}): {e}", exc_info=True)

delete_knowledge_task.max_tries = 3 # type: ignore
delete_knowledge_task.retry_delay = 2 # type: ignore

async def generate_testset_task(ctx: Any, testset_id: int, source_doc_ids: List[int], generator_model: str = "qwen-max"):
    logger.info(f"[Task] å¼€å§‹ç”Ÿæˆæµ‹è¯•é›†: ID {testset_id}")
    async with async_session_maker() as db:
        try:
            await generate_testset_pipeline(db, testset_id, source_doc_ids, generator_model)
        except Exception as e:
            logger.error(f"[Task] æµ‹è¯•é›†ç”Ÿæˆå¼‚å¸¸ (ID {testset_id}): {e}", exc_info=True)

generate_testset_task.max_tries = 3 # type: ignore
generate_testset_task.retry_delay = 10 # type: ignore

async def run_experiment_task(ctx: Any, experiment_id: int):
    logger.info(f"[Task] å¼€å§‹è¿è¡Œå®éªŒ: ID {experiment_id}")
    async with async_session_maker() as db:
        try:
            await run_experiment_pipeline(db, experiment_id)
        except Exception as e:
            logger.error(f"[Task] å®éªŒè¿è¡Œå¼‚å¸¸ (ID {experiment_id}): {e}", exc_info=True)

run_experiment_task.max_tries = 3 # type: ignore
run_experiment_task.retry_delay = 10 # type: ignore

# --- Arq é…ç½® ---

class WorkerSettings:
    functions = [
        process_document_task, 
        delete_knowledge_task, 
        generate_testset_task, 
        run_experiment_task
    ]
    redis_settings = RedisSettings(
        host=settings.REDIS_HOST, 
        port=settings.REDIS_PORT
        )
    
    queue_name = os.getenv("ARQ_QUEUES", settings.DEFAULT_QUEUE_NAME)
    max_jobs = 1
    on_startup = startup
    on_shutdown = shutdown