import json
import logging

from fastapi import APIRouter, Depends
from fastapi.responses import StreamingResponse

from app.api.deps import get_rag_pipeline_factory
from app.domain.schemas import QueryRequest, QueryResponse, Source
from app.services.generation import QAService
from app.services.pipelines import RAGPipeline
from app.core.config import settings    
from app.api import deps
from app.services.retrieval.vector_store_manager import VectorStoreManager
from app.services.retrieval import RetrievalService
from typing import List
from langchain_core.documents import Document
from typing import Any

logger = logging.getLogger(__name__)

router = APIRouter()
@router.post("/query", response_model=QueryResponse)
async def handle_query(
    request: QueryRequest,
    # ç»Ÿä¸€ä½¿ç”¨å·¥å‚ï¼Œä¸å†éœ€è¦å•ç‹¬æ³¨å…¥ store_manager æˆ– qa_service
    pipeline_factory = Depends(deps.get_rag_pipeline_factory),
):  
    # 1. ä¸€è¡Œä»£ç åˆ›å»º Pipelineï¼Œç­–ç•¥é€»è¾‘è¢«å°è£…äº†
    rag_chain = pipeline_factory(
        knowledge_id=request.knowledge_id,
        strategy=request.strategy, # ğŸ‘ˆ ä¼ å…¥å‰ç«¯è¯·æ±‚çš„ç­–ç•¥
        top_k=settings.TOP_K
    )

    # 2. æ‰§è¡ŒæŸ¥è¯¢
    # async_query è¿”å› (answer, docs) å…ƒç»„
    answer, docs = await rag_chain.async_query(request.query)

    # 3. æ ¼å¼åŒ–æ¥æº (ä¿æŒåŸæœ‰é€»è¾‘)
    sources_list = []
    for doc in docs:
        # ... (åŸæœ‰çš„ metadata æå–ä»£ç ä¿æŒä¸å˜) ...
        metadata = doc.metadata
        sources_list.append(Source(
            source_filename=metadata.get("source", "æœªçŸ¥æ–‡ä»¶"),
            page_number=metadata.get("page"),
            chunk_content=doc.page_content,
            chunk_id=str(metadata.get("doc_id"))
        ))

    return QueryResponse(
        answer=answer,
        sources=sources_list
    )


@router.post("/stream")
async def stream_query(
    request: QueryRequest,
    pipeline_factory = Depends(get_rag_pipeline_factory),
):
    """
    SSE (Server-Sent Events) æµå¼è¿”å›ã€‚
    äº‹ä»¶æµé¡ºåº:
    1. event: sources \n data: [JSON List of Sources]
    2. event: message \n data: "Token String"  <-- ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ JSON å­—ç¬¦ä¸²
    ...
    """
    rag_chain = pipeline_factory(
        knowledge_id=request.knowledge_id,
        strategy=request.strategy,
        top_k=settings.TOP_K
    )
    
    async def event_generator():
        logger.debug(f"æ”¶åˆ° Stream API æŸ¥è¯¢: {request.query}")        
        
        # è¿­ä»£å™¨ä¼šå…ˆè¿”å› List[Document]ï¼Œç„¶åè¿”å› str (token)
        async for chunk in rag_chain.astream_with_sources(request.query):
            
            # å¦‚æœæ˜¯æ–‡æ¡£åˆ—è¡¨ï¼Œæ„é€  sources äº‹ä»¶
            if isinstance(chunk, list):
                sources_data = []
                for doc in chunk:
                    metadata = doc.metadata
                    src = Source(
                        source_filename=metadata.get("source", "æœªçŸ¥æ–‡ä»¶"),
                        page_number=metadata.get("page"),
                        chunk_content=doc.page_content,
                        chunk_id=str(metadata.get("doc_id"))
                    )
                    # Pydantic model_dump éœ€è¦ mode='json' æ¥å¤„ç†ä¸€äº›ç‰¹æ®Šç±»å‹
                    sources_data.append(src.model_dump(mode='json'))
                
                # å‘é€ sources äº‹ä»¶
                yield f"event: sources\ndata: {json.dumps(sources_data, ensure_ascii=False)}\n\n"
            
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ„é€  message äº‹ä»¶ (æˆ–è€…ç›´æ¥ data)
            elif isinstance(chunk, str):
                # [ä¿®æ”¹] ä½¿ç”¨ json.dumps åŒ…è£… chunkï¼Œä¿æŠ¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
                yield f"event: message\ndata: {json.dumps(chunk)}\n\n"
    
    return StreamingResponse(event_generator(), media_type="text/event-stream")