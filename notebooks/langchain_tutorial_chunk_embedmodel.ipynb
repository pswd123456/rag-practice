{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab316cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGSMITH_API_KEY 已成功加载！\n",
      "Key: lsv2...\n",
      "Tracing 状态: true\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 关键：这一行代码会去查找 .env 文件，并把里面的键值对加载到 os.environ 中\n",
    "load_dotenv() \n",
    "\n",
    "# --- 加载完成后，你就不需要你原先那两行代码了 ---\n",
    "# 删掉：os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# 删掉：os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# 你现在可以检查一下变量是否已经成功加载\n",
    "deepseek_apikey = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "if not deepseek_apikey:\n",
    "    print(\"错误：未能在 .env 文件中找到 DEEPSEEK_API_KEY。\")\n",
    "\n",
    "key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "tracing = os.getenv(\"LANGSMITH_TRACING\")\n",
    "base_url=\"https://api.deepseek.com\"\n",
    "\n",
    "if key:\n",
    "    print(\"LANGSMITH_API_KEY 已成功加载！\")\n",
    "    # 为了安全，只打印一部分\n",
    "    print(f\"Key: {key[:4]}...\") \n",
    "else:\n",
    "    print(\"错误：未能在 .env 文件中找到 LANGSMITH_API_KEY。\")\n",
    "\n",
    "print(f\"Tracing 状态: {tracing}\")\n",
    "\n",
    "# LangChain 的代码现在就可以正常工作了\n",
    "# ... 你的 LangChain/LangSmith 初始化代码 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003f5521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\86185\\miniconda3\\envs\\naive-rag-practice\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"../data/rag.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca3c6780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(all_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2653cf10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 运行这个命令，看它是否返回 True\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e55d566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2060'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf5acf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86185\\AppData\\Local\\Temp\\ipykernel_30504\\3469660604.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa31b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['194c90df-003d-47ee-9465-2afe2a99d110',\n",
       " '1c9e83ec-22cd-4f4d-b4cf-a28112a10ed4',\n",
       " '7473bea1-2745-49d7-ab05-7fb097a22f6e',\n",
       " 'c464ef6b-12de-42ae-a792-ac1f85aab091',\n",
       " '0dfa60b6-eea1-4a5b-92a6-aa3e8242d299',\n",
       " '16af46c1-0999-40ce-9efd-85e3d4550151',\n",
       " 'ae047841-8c34-4e78-91f4-cf698bf9e534',\n",
       " '53f850a2-553d-4e56-a4c9-28724ac68ec0',\n",
       " '86f1dae0-455c-44c6-bc79-d99110071d8f',\n",
       " '9e7b5747-314a-4b54-96e6-2f5689153969',\n",
       " '64cc0e51-e907-46f1-b0fe-f15ebf54acac',\n",
       " '1229386b-9c9d-4083-8907-88612265b3ed',\n",
       " '1209c2dd-ab5e-4f00-a601-562e9cc9d6d8',\n",
       " '99c1ce32-8ec0-42e5-a919-6d0bc5850b6b',\n",
       " '06c726b1-a338-49e6-92c7-b9bd3412f3f0',\n",
       " 'c5776851-2aac-4d41-8917-d57740dc4cbc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store_1000_qwen = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"../db/chroma_chunk_1000_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "vector_store_1000_qwen.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b79d1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "def retriever(query: str) -> List[Document]:\n",
    "    return vector_store_1000_qwen.similarity_search(query, k=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77d95a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG 链创建成功！\n",
      "first={\n",
      "  context: RunnableLambda(retriever)\n",
      "           | RunnableLambda(format_docs),\n",
      "  question: RunnablePassthrough()\n",
      "} middle=[ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n你是一个问答助手。请根据下面提供的“上下文”来回答问题。\\n如果你在上下文中找不到答案，就说你不知道。\\n\\n上下文:\\n{context}\\n\\n问题:\\n{question}\\n'), additional_kwargs={})]), ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001DECDC81420>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001DF2E48E680>, root_client=<openai.OpenAI object at 0x000001DECDC80EE0>, root_async_client=<openai.AsyncOpenAI object at 0x000001DF2E48E650>, model_name='deepseek-chat', model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.deepseek.com')] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 导入必要的“链接”工具 ---\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI # 注意：使用 ChatOpenAI，而不是 OpenAI\n",
    "\n",
    "# --- 初始化的DeepSeek 模型 ---\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=deepseek_apikey, \n",
    "    base_url=\"https://api.deepseek.com\" \n",
    ")\n",
    "\n",
    "# --- 定义 RAG 链 ---\n",
    "# A. Prompt 模板：告诉 LLM 如何使用上下文\n",
    "template = \"\"\"\n",
    "你是一个问答助手。请根据下面提供的“上下文”来回答问题。\n",
    "如果你在上下文中找不到答案，就说你不知道。\n",
    "\n",
    "上下文:\n",
    "{context}\n",
    "\n",
    "问题:\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# B. 定义格式化函数：把 Document 列表转换成普通字符串\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# C. 把所有“零件”用“|”(管道)串联起来\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✅ RAG 链创建成功！\")\n",
    "print(rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b12cb407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 运行这条链！\n",
    "# question = \"今天早上吃什么\"\n",
    "# response = rag_chain.invoke(question)\n",
    "\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e943d3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['773f9386-60d4-4ac0-85af-fae71065d531',\n",
       " 'fd963fda-a299-4f56-b05b-ec43928d47d7',\n",
       " '8f6b3ab9-0a74-4760-ace9-10c52da238fc',\n",
       " '9c48ac48-6c18-4c75-8926-ab1c4c410742',\n",
       " 'a56d5314-8694-4680-8842-c43019fd6e21',\n",
       " 'c7032830-8b6a-4db5-bbd3-e11f761eb666',\n",
       " 'f22b2da9-ef40-4693-af71-d98ba8d31fa0',\n",
       " '9aff31b6-f956-484d-91bc-64761de7f8de',\n",
       " 'bae53c4f-d884-4cae-8d39-f3169652608c',\n",
       " '829d25c4-a6ed-467d-855e-6d4f36d2cc23',\n",
       " '12513ff8-4ed7-4df6-a587-87a2b8e7f546',\n",
       " '8e587783-7c3e-4d89-960c-e7ae7f18f742',\n",
       " '840328c4-04f7-408a-8bf8-edae19e4bcfb',\n",
       " 'c3d125ad-03bd-4078-9b5c-1443d7198158',\n",
       " '68295870-40c0-41f2-aecc-cd3ca15995ba',\n",
       " '43263ef2-d081-4640-8f7f-1b7cc452d3ad',\n",
       " '884d693b-b186-491b-ad7d-aa9d985dd188',\n",
       " '9ad65168-6a3d-431e-ab81-b684a3966237',\n",
       " '0fd5de33-3728-40de-8557-2a49e65ca79d',\n",
       " 'f11ec5b5-aa42-41ab-83e5-6d4ecbdd1e41',\n",
       " '241d5ba2-8b30-4402-879b-4ae3ed26569d',\n",
       " 'b5fdd203-c5bc-42e7-a7b0-edeb5799710b',\n",
       " 'e151f26d-810d-4c27-abba-a6a45c778d64',\n",
       " '3b29b06f-220c-4bcd-8789-71bf6d143d31',\n",
       " 'e0cff7cf-1563-4fdf-aef9-242a04c82af2',\n",
       " 'bad61e30-c4c9-4641-9cb6-d1c2d5aa1c01',\n",
       " '74107356-5019-42b1-879c-d8036993dc23',\n",
       " '305803b0-d476-459f-931f-9a3937d46c59',\n",
       " '0b505dba-ca6e-4953-92d9-3e8b75d12f5e',\n",
       " 'fb93c5c3-28ef-4411-879a-8e0ec08558b5',\n",
       " '4bf77f47-380e-41bf-9879-3ba3ef5b53bc',\n",
       " 'b3a13797-138d-42fc-a048-422406ace037',\n",
       " '947a334e-0ac4-4631-9c9c-68dee31c38a2',\n",
       " '2ac49b24-36a6-42ac-b29e-5657b18a87c8',\n",
       " '742fe296-6959-42b9-9f84-8288ac25a307',\n",
       " '10e521da-24f7-4174-8360-73d1f8d0a261',\n",
       " '9ffde90d-01c3-4473-8a4a-f6e2867eca8d',\n",
       " '0251cff9-d80f-4c2d-bb7d-ff33d0429b8f',\n",
       " 'bab62434-d350-4438-96f8-ac2ce5171ea3',\n",
       " '0533c9f3-25de-49dd-8d2f-0efaa3dffcc0',\n",
       " 'a2b834ff-5a0d-44b2-b40c-eecc804d4658',\n",
       " 'a7c32f8a-e5be-4076-a2b7-93d4963e3aa6',\n",
       " 'f12350c2-bdd7-499d-8773-3f2755d33d99',\n",
       " '0bb55345-8381-4dbb-98e8-c5bf133752e5',\n",
       " '9be369e2-40c2-46ff-bfd8-df9a738f0d83',\n",
       " 'a0abc20a-44b5-4421-9794-b3f47734f3b3',\n",
       " '4e8b8119-6dd9-4e38-aadf-64c5f5947735']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实验组 A：小块\n",
    "text_splitter_300 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, chunk_overlap=50, add_start_index=True\n",
    ")\n",
    "all_splits_300 = text_splitter_300.split_documents(docs) # 'docs' 来自 [cell 2]\n",
    "\n",
    "# 关键：创建一个*新*的数据库集合来存放\n",
    "vector_store_300 = Chroma(\n",
    "    collection_name=\"rag_pdf_chunk_300\", # 新的名字\n",
    "    embedding_function=embeddings, # 还是用 Qwen\n",
    "    persist_directory=\"../db/chroma_chunk_300_db\", # 新的目录\n",
    ")\n",
    "vector_store_300.add_documents(documents=all_splits_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a142d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1000 Chunk 结果 ---\n",
      "[Document(id='1c9e83ec-22cd-4f4d-b4cf-a28112a10ed4', metadata={'source': '../data/rag.pdf', 'total_pages': 5, 'page_label': '1', 'creationdate': '2025-10-31T03:11:04+08:00', 'page': 0, 'producer': 'jsPDF 3.0.1', 'creator': 'PyPDF', 'start_index': 758}, page_content='relevant text from databases, uploaded documents, or web  sources.[1] According to Ars \\nTechnica, \"RAG  is a way of improving LLM  performance, in essence by blending the LLM  \\nprocess with a web  search or other document look-up process to help LLMs  stick to the facts.\"\\nThis method helps reduce AI hallucinations,[3] which have caused chatbots to describe \\npolicies that don\\'t exist, or recommend  nonexistent legal cases to lawyers that are looking for \\ncitations to support their arguments.[4]\\nRAG  also reduces the need to retrain LLMs  with new  data, saving on computational and \\nfinancial costs.[1] Beyond efficiency gains, RAG  also allows LLMs  to include sources in their \\nresponses, so users can verify the cited sources. This provides greater transparency, as users\\ncan cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG  was first introduced in a 2020 research paper.[3]\\nRAG  and LLM  Limitations')]\n",
      "--- 300 Chunk 结果 ---\n",
      "[Document(id='9c48ac48-6c18-4c75-8926-ab1c4c410742', metadata={'source': '../data/rag.pdf', 'total_pages': 5, 'producer': 'jsPDF 3.0.1', 'start_index': 758, 'page': 0, 'page_label': '1', 'creationdate': '2025-10-31T03:11:04+08:00', 'creator': 'PyPDF'}, page_content='relevant text from databases, uploaded documents, or web  sources.[1] According to Ars \\nTechnica, \"RAG  is a way of improving LLM  performance, in essence by blending the LLM  \\nprocess with a web  search or other document look-up process to help LLMs  stick to the facts.\"')]\n"
     ]
    }
   ],
   "source": [
    "# 原始查询 (k=1)\n",
    "retriever_1000 = vector_store_1000_qwen.as_retriever(search_kwargs={\"k\": 1})\n",
    "print(\"--- 1000 Chunk 结果 ---\")\n",
    "print(retriever_1000.invoke(\"What did Ars Technica say about RAG?\"))\n",
    "\n",
    "# 新查询 (k=1)\n",
    "retriever_300 = vector_store_300.as_retriever(search_kwargs={\"k\": 1})\n",
    "print(\"--- 300 Chunk 结果 ---\")\n",
    "print(retriever_300.invoke(\"What did Ars Technica say about RAG?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c0860",
   "metadata": {},
   "source": [
    "你现在必须站在**下一步（LLM，即你的 DeepSeek 模型）**的角度来思考。\n",
    "\n",
    "对于 300 块 (Chunk=300) 的结果：\n",
    "\n",
    "你（作为 DeepSeek）收到的上下文是：\"relevant text... Ars Technica...\"\n",
    "\n",
    "这是一个极其干净、高度聚焦的“小纸条”。\n",
    "\n",
    "你的任务很简单：“嘿，DeepSeek，请总结一下 Ars Technica 说了什么。”\n",
    "\n",
    "结果： DeepSeek 会给你一个非常棒的、直接的答案。\n",
    "\n",
    "对于 1000 块 (Chunk=1000) 的结果：\n",
    "\n",
    "你（作为 DeepSeek）收到的上下文是：\n",
    "\n",
    "\"relevant text... Ars Technica...\"（这是你要的答案）\n",
    "\n",
    "\"...This method helps reduce AI hallucinations...\"（这是无关噪音）\n",
    "\n",
    "\"...RAG also reduces the need to retrain LLMs...\"（这是无关噪音）\n",
    "\n",
    "\"...RAG was first introduced in a 2020...\"（这是无关噪音）\n",
    "\n",
    "这是一个**非常“吵闹”、充满“噪音”**的“大文档”。\n",
    "\n",
    "结果： DeepSeek 可能会“犯迷糊”。它在回答“Ars Technica 说了什么”时，很有可能会被后面“减少幻觉”、“2020年论文”等噪音带偏，给你一个冗长且跑题的答案。\n",
    "\n",
    "总结\n",
    "你这个实验完美证明了那个为期 2 天的计划的核心：\n",
    "\n",
    "Chunking (分块) 是 RAG 中最重要的“权衡” (Trade-off) 之一。\n",
    "\n",
    "小块 (Chunk=300)： 在回答**“具体细节”问题时，能提供更干净、更精确**的上下文。\n",
    "\n",
    "大块 (Chunk=1000)： 可能会提供**“过多”的上下文，反而“污染”**了 LLM 的思考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5872abe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1000 Chunk 结果 ---\n",
      "[Document(id='194c90df-003d-47ee-9465-2afe2a99d110', metadata={'page_label': '1', 'source': '../data/rag.pdf', 'start_index': 0, 'page': 0, 'producer': 'jsPDF 3.0.1', 'creator': 'PyPDF', 'total_pages': 5, 'creationdate': '2025-10-31T03:11:04+08:00'}, page_content='Retrieval-augmented generation (RAG) is a technique that enables large language models \\n(LLMs) to retrieve and incorporate new  information.[1] With RAG,  LLMs  do not respond to user\\nqueries until they refer to a specified set of documents. These documents supplement \\ninformation from the LLM\\'s pre-existing training data.[2] This allows LLMs  to use \\ndomain-specific and/or updated information that is not available in the training data.[2] For \\nexample, this helps LLM-based chatbots access internal company data or generate responses\\nbased on authoritative sources.\\nRAG  improves large language models (LLMs) by incorporating information retrieval before \\ngenerating responses.[3] Unlike traditional LLMs  that rely on static training data, RAG  pulls \\nrelevant text from databases, uploaded documents, or web  sources.[1] According to Ars \\nTechnica, \"RAG  is a way of improving LLM  performance, in essence by blending the LLM')]\n",
      "--- 300 Chunk 结果 ---\n",
      "[Document(id='85705d95-f94b-4c1c-a903-2ad218abf6b6', metadata={'page': 1, 'creator': 'PyPDF', 'total_pages': 5, 'source': '../data/rag.pdf', 'creationdate': '2025-10-31T03:11:04+08:00', 'producer': 'jsPDF 3.0.1', 'page_label': '2', 'start_index': 526}, page_content='augmented prompt and its internal representation of its training data to synthesize an \\nengaging answer tailored to the user in that instant\".[1]\\nRAG  key stages\\nOverview of RAG  process, combining external documents and user input into an LLM  prompt \\nto get tailored output')]\n"
     ]
    }
   ],
   "source": [
    "# 原始查询 (k=1)\n",
    "retriever_1000 = vector_store_1000_qwen.as_retriever(search_kwargs={\"k\": 1})\n",
    "print(\"--- 1000 Chunk 结果 ---\")\n",
    "print(retriever_1000.invoke(\"What is RAG and give me an example?\"))\n",
    "\n",
    "# 新查询 (k=1)\n",
    "retriever_300 = vector_store_300.as_retriever(search_kwargs={\"k\": 1})\n",
    "print(\"--- 300 Chunk 结果 ---\")\n",
    "print(retriever_300.invoke(\"What is RAG and give me an example?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fd0f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\86185\\miniconda3\\envs\\llm_env\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 2.4/6.2 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.2 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 10.0 MB/s  0:00:00\n",
      "Installing collected packages: sympy\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "Successfully installed sympy-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0832dfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fefbeacd-e3c0-4df4-b831-7f2336d6961a',\n",
       " 'ba4398b7-88cd-481a-8885-3a23173a6dda',\n",
       " 'dca30fa8-05e4-4694-b225-508f34c35229',\n",
       " 'cec2561a-073c-4178-8de0-e5dd58caa510',\n",
       " 'cc400814-6df9-4692-97e6-aae7b0134e8c',\n",
       " '5dc88865-52ce-4c6b-8898-d3d0972acbb0',\n",
       " 'c0d759a7-11c6-4442-a5b4-cf8e59c58332',\n",
       " '4f58d890-b5ea-47e6-9165-7987bb3c7ac6',\n",
       " '0efb8ad4-0858-4bbc-8c37-60166a16d7fc',\n",
       " 'aa72ea73-32b2-4833-b639-a5a757851421',\n",
       " '634bcc74-bea3-4d28-a107-93df487175d4',\n",
       " 'cf3963ce-0633-4f65-be6c-e68270134c75',\n",
       " '1d57b038-6b7b-4668-ba53-a66a6434968a',\n",
       " '93a9b0ef-dafd-4e91-b318-56b3541116ea',\n",
       " 'fc0944aa-ad52-4147-94d1-53ab3b0f5e13',\n",
       " 'cf73cdba-9fdb-438a-9510-08b94bc8a4a7']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实验组 B：新模型\n",
    "model_name_mini = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "embeddings_mini = HuggingFaceEmbeddings(\n",
    "    model_name=model_name_mini,\n",
    "    model_kwargs={'device': 'cuda'} # 同样用 GPU\n",
    ")\n",
    "\n",
    "# 用回你 1000 大小的切块\n",
    "vector_store_mini = Chroma(\n",
    "    collection_name=\"rag_pdf_minilm\", # 新的名字\n",
    "    embedding_function=embeddings_mini, # 传入新模型\n",
    "    persist_directory=\"../db/chroma_minilm_db\", # 新的目录\n",
    ")\n",
    "vector_store_mini.add_documents(documents=all_splits) # 'all_splits' 来自 [cell 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e9aae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Qwen 结果 (Score: 0.4299754798412323) ---\n",
      "Retrieval-augmented generation (RAG) is a technique that enables large language models \n",
      "(LLMs) to retrieve and incorporate new  information.[1] With RAG,  LLMs  do not respond to user\n",
      "queries until they refer to a specified set of documents. These documents supplement \n",
      "information from the LLM's pre-existing training data.[2] This allows LLMs  to use \n",
      "domain-specific and/or updated information that is not available in the training data.[2] For \n",
      "example, this helps LLM-based chatbots access internal company data or generate responses\n",
      "based on authoritative sources.\n",
      "RAG  improves large language models (LLMs) by incorporating information retrieval before \n",
      "generating responses.[3] Unlike traditional LLMs  that rely on static training data, RAG  pulls \n",
      "relevant text from databases, uploaded documents, or web  sources.[1] According to Ars \n",
      "Technica, \"RAG  is a way of improving LLM  performance, in essence by blending the LLM\n",
      "--- MiniLM 结果 (Score: 1.060840368270874) ---\n",
      "relevant text from databases, uploaded documents, or web  sources.[1] According to Ars \n",
      "Technica, \"RAG  is a way of improving LLM  performance, in essence by blending the LLM  \n",
      "process with a web  search or other document look-up process to help LLMs  stick to the facts.\"\n",
      "This method helps reduce AI hallucinations,[3] which have caused chatbots to describe \n",
      "policies that don't exist, or recommend  nonexistent legal cases to lawyers that are looking for \n",
      "citations to support their arguments.[4]\n",
      "RAG  also reduces the need to retrain LLMs  with new  data, saving on computational and \n",
      "financial costs.[1] Beyond efficiency gains, RAG  also allows LLMs  to include sources in their \n",
      "responses, so users can verify the cited sources. This provides greater transparency, as users\n",
      "can cross-check retrieved content to ensure accuracy and relevance.\n",
      "The term RAG  was first introduced in a 2020 research paper.[3]\n",
      "RAG  and LLM  Limitations\n"
     ]
    }
   ],
   "source": [
    "# Qwen (1024维)\n",
    "results_qwen = vector_store_1000_qwen.similarity_search_with_score(\"What is RAG and give me an example?\")\n",
    "print(f\"--- Qwen 结果 (Score: {results_qwen[0][1]}) ---\")\n",
    "print(results_qwen[0][0].page_content)\n",
    "\n",
    "# MiniLM (384维)\n",
    "results_mini = vector_store_mini.similarity_search_with_score(\"What is RAG and give me an example?\")\n",
    "print(f\"--- MiniLM 结果 (Score: {results_mini[0][1]}) ---\")\n",
    "print(results_mini[0][0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf60ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naive-rag-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
